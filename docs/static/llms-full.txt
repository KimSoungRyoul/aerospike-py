# aerospike-py

> High-performance Aerospike Python Client built in Rust (Sync/Async). Provides both synchronous and asynchronous APIs with zero-copy NumPy batch reads, OpenTelemetry tracing, and Prometheus metrics.

## Getting Started

### Contributing

> Development setup, build, test, and code style guidelines.

#### Setup

```bash
git clone https://github.com/KimSoungRyoul/aerospike-py.git
cd aerospike-py
make install          # uv sync --all-groups
make build            # uv run maturin develop --release
```

#### Start Aerospike

```bash
make run-aerospike-ce   # Aerospike CE on port 18710
```

#### Build

```bash
make build                    # Release build (recommended)
maturin develop               # Debug build (faster compile)
maturin build --release       # Build wheel
```

#### Test

```bash
make test-unit          # No server needed
make test-integration   # Server needed
make test-all           # All tests
```

#### Lint & Format

```bash
make lint     # ruff check + cargo clippy
make fmt      # ruff format + cargo fmt
```

#### Pre-commit

```bash
pip install pre-commit
pre-commit install
```

#### Project Structure

```
aerospike-py/
├── rust/src/               # PyO3 Rust bindings
│   ├── client.rs           # Sync Client
│   ├── async_client.rs     # Async Client
│   ├── errors.rs           # Error → Exception
│   ├── types/              # Type converters
│   └── policy/             # Policy parsers
├── src/aerospike_py/       # Python package
├── tests/                  # unit/ integration/ concurrency/ compatibility/
├── docs/                   # Docusaurus
└── benchmark/              # Benchmarks
```

#### Making Changes

1. **Rust** (`rust/src/`): Edit, then `maturin develop` to rebuild
2. **Python** (`src/aerospike_py/`): Changes apply immediately
3. **Tests**: Add to `tests/unit/` or `tests/integration/`
4. **Docs**: Edit `docs/docs/`, preview with `cd docs && npm start`

#### Architecture Notes

- **Sync Client**: Global Tokio runtime, `py.allow_threads(|| RUNTIME.block_on(...))` releases GIL
- **Async Client**: `pyo3_async_runtimes::tokio::future_into_py()` returns Python coroutines
- **Type conversion**: Python ↔ Rust `Value` enum in `types/value.rs`
- **Error mapping**: `aerospike_core::Error` → Python exceptions in `errors.rs`


---

### Getting Started

> Install aerospike-py and connect to an Aerospike cluster in minutes.

#### Installation

```bash
pip install aerospike-py
```

**Requirements:** Python 3.10+ (CPython) | Linux (x86_64, aarch64) | macOS (x86_64, arm64) | Windows (x64)

#### Quick Start

##### Sync

```python
import aerospike_py as aerospike
from aerospike_py import Record

with aerospike.client({
    "hosts": [("127.0.0.1", 3000)],
}).connect() as client:
    key: tuple[str, str, str] = ("test", "demo", "user1")

    # Write
    client.put(key, {"name": "Alice", "age": 30})

    # Read
    record: Record = client.get(key)
    print(record.bins)       # {"name": "Alice", "age": 30}
    print(record.meta.gen)   # 1

    # Update
    client.increment(key, "age", 1)

    # Delete
    client.remove(key)
```

##### Async

```python
import asyncio
import aerospike_py as aerospike
from aerospike_py import AsyncClient, Record

async def main() -> None:
    async with AsyncClient({"hosts": [("127.0.0.1", 3000)]}) as client:
        await client.connect()
        key: tuple[str, str, str] = ("test", "demo", "user1")

        await client.put(key, {"name": "Bob", "age": 25})

        record: Record = await client.get(key)
        print(record.bins)  # {"name": "Bob", "age": 25}

        # Concurrent writes
        keys = [("test", "demo", f"item_{i}") for i in range(10)]
        await asyncio.gather(*(client.put(k, {"idx": i}) for i, k in enumerate(keys)))

        await client.remove(key)

asyncio.run(main())
```

#### Policies & Metadata

```python
import aerospike_py as aerospike

key = ("test", "demo", "user1")

# TTL (seconds)
client.put(key, {"val": 1}, meta={"ttl": 300})

# Create only (fail if exists)
client.put(key, {"val": 1}, policy={"exists": aerospike.POLICY_EXISTS_CREATE_ONLY})

# Optimistic locking
record = client.get(key)
client.put(
    key,
    {"val": record.bins["val"] + 1},
    meta={"gen": record.meta.gen},
    policy={"gen": aerospike.POLICY_GEN_EQ},
)
```

#### Next Steps

| Topic | Description |
|-------|-------------|
| [Read Operations](/docs/guides/crud/read) | Get, select, exists, batch read |
| [Write Operations](/docs/guides/crud/write) | Put, update, delete, operate, batch operate |
| [CDT Operations](/docs/guides/crud/operations) | Atomic list & map operations |
| [NumPy Batch](/docs/guides/crud/numpy-batch) | Zero-copy columnar batch reads |
| [Query](/docs/guides/query-scan/query-scan) | Secondary index queries |
| [Expression Filters](/docs/guides/query-scan/expression-filters) | Server-side filtering |
| [Configuration](/docs/guides/config/client-config) | Connection, pool, timeouts |
| [API Reference](/docs/api/client) | Full method signatures |
| [Types](/docs/api/types) | NamedTuple / TypedDict definitions |


---

## API Reference

### Client

> Complete API reference for the synchronous Client and asynchronous AsyncClient classes.

<!-- AUTO-GENERATED from .pyi docstrings. Do not edit manually. -->

aerospike-py provides both synchronous (`Client`) and asynchronous (`AsyncClient`) APIs with identical functionality.

#### Factory Functions

##### `client(config)`

Create a new Aerospike client instance.

| Parameter | Description |
|-----------|-------------|
| `config` | [`ClientConfig`](types.md#clientconfig) dictionary. Must contain a ``"hosts"`` key with a list of ``(host, port)`` tuples. |

**Returns:** A new ``Client`` instance (not yet connected).

```python
import aerospike_py

client = aerospike_py.client({
    "hosts": [("127.0.0.1", 3000)],
}).connect()
```

##### `set_log_level(level)`

Set the aerospike_py log level.

Accepts ``LOG_LEVEL_*`` constants. Controls both Rust-internal
and Python-side logging.

| Parameter | Description |
|-----------|-------------|
| `level` | One of ``LOG_LEVEL_OFF`` (-1), ``LOG_LEVEL_ERROR`` (0), ``LOG_LEVEL_WARN`` (1), ``LOG_LEVEL_INFO`` (2), ``LOG_LEVEL_DEBUG`` (3), ``LOG_LEVEL_TRACE`` (4). |

```python
import aerospike_py

aerospike_py.set_log_level(aerospike_py.LOG_LEVEL_DEBUG)
```

##### `get_metrics()`

Return collected metrics in Prometheus text format.

**Returns:** A string in Prometheus exposition format.

```python
print(aerospike_py.get_metrics())
```

##### `start_metrics_server(port=9464)`

Start a background HTTP server serving ``/metrics`` for Prometheus.

| Parameter | Description |
|-----------|-------------|
| `port` | TCP port to listen on (default ``9464``). |

```python
aerospike_py.start_metrics_server(port=9464)
```

##### `stop_metrics_server()`

Stop the background metrics HTTP server.

```python
aerospike_py.stop_metrics_server()
```

#### Connection

##### `connect(username=None, password=None)`

Connect to the Aerospike cluster.

Returns ``self`` for method chaining.

| Parameter | Description |
|-----------|-------------|
| `username` | Optional username for authentication. |
| `password` | Optional password for authentication. |

**Returns:** The connected client instance.

> **Note:**
> Raises `ClusterError` Failed to connect to any cluster node.
##### Sync Client

```python
client = aerospike_py.client(config).connect()

# With authentication
client = aerospike_py.client(config).connect("admin", "admin")
```

##### Async Client

```python
client = await aerospike_py.AsyncClient(config).connect()
await client.connect("admin", "admin")
```

##### `is_connected()`

Check whether the client is connected to the cluster.

**Returns:** ``True`` if the client has an active cluster connection.

##### Sync Client

```python
if client.is_connected():
    print("Connected")
```

##### Async Client

```python
if client.is_connected():
    print("Connected")
```

##### `close()`

Close the connection to the cluster.

After calling this method the client can no longer be used for
database operations.

##### Sync Client

```python
client.close()
```

##### Async Client

```python
await client.close()
```

##### `get_node_names()`

Return the names of all nodes in the cluster.

**Returns:** A list of node name strings.

##### Sync Client

```python
nodes = client.get_node_names()
# ['BB9020011AC4202', 'BB9030011AC4202']
```

##### Async Client

```python
nodes = await client.get_node_names()
```

#### Info

##### `info_all(command, policy=None)`

Send an info command to all cluster nodes.

| Parameter | Description |
|-----------|-------------|
| `command` | The info command string (e.g. ``"namespaces"``). |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

**Returns:** A list of ``InfoNodeResult(node_name, error_code, response)`` tuples.

##### Sync Client

```python
results = client.info_all("namespaces")
for node, err, response in results:
    print(f"{node}: {response}")
```

##### Async Client

```python
results = await client.info_all("namespaces")
for node, err, response in results:
    print(f"{node}: {response}")
```

##### `info_random_node(command, policy=None)`

Send an info command to a random cluster node.

| Parameter | Description |
|-----------|-------------|
| `command` | The info command string. |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

**Returns:** The info response string.

##### Sync Client

```python
response = client.info_random_node("build")
```

##### Async Client

```python
response = await client.info_random_node("build")
```

#### CRUD Operations

##### `put(key, bins, meta=None, policy=None)`

Write a record to the Aerospike cluster.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `bins` | Dictionary of bin name-value pairs to write. |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict (e.g. ``{"ttl": 300}``). |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

> **Note:**
> Raises `RecordExistsError` Record already exists (with CREATE_ONLY policy).
> **Note:**
> Raises `RecordTooBig` Record size exceeds the configured write-block-size.
##### Sync Client

```python
key = ("test", "demo", "user1")
client.put(key, {"name": "Alice", "age": 30})

# With TTL (seconds)
client.put(key, {"score": 100}, meta={"ttl": 300})

# Create only (fail if exists)
import aerospike_py
client.put(
    key,
    {"x": 1},
    policy={"exists": aerospike_py.POLICY_EXISTS_CREATE_ONLY},
)
```

##### Async Client

```python
key = ("test", "demo", "user1")
await client.put(key, {"name": "Alice", "age": 30})

# With TTL (seconds)
await client.put(key, {"score": 100}, meta={"ttl": 300})
```

##### `get(key, policy=None)`

Read a record from the cluster.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `policy` | Optional [`ReadPolicy`](types.md#readpolicy) dict. |

**Returns:** A ``Record`` NamedTuple with ``key``, ``meta``, ``bins`` fields.

> **Note:**
> Raises `RecordNotFound` The record does not exist.
##### Sync Client

```python
record = client.get(("test", "demo", "user1"))
print(record.bins)  # {"name": "Alice", "age": 30}
```

##### Async Client

```python
record = await client.get(("test", "demo", "user1"))
print(record.bins)  # {"name": "Alice", "age": 30}
```

##### `select(key, bins, policy=None)`

Read specific bins from a record.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `bins` | List of bin names to retrieve. |
| `policy` | Optional [`ReadPolicy`](types.md#readpolicy) dict. |

**Returns:** A ``Record`` NamedTuple with ``key``, ``meta``, ``bins`` fields.

> **Note:**
> Raises `RecordNotFound` The record does not exist.
##### Sync Client

```python
record = client.select(("test", "demo", "user1"), ["name"])
# record.bins = {"name": "Alice"}
```

##### Async Client

```python
record = await client.select(("test", "demo", "user1"), ["name"])
# record.bins = {"name": "Alice"}
```

##### `exists(key, policy=None)`

Check whether a record exists.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `policy` | Optional [`ReadPolicy`](types.md#readpolicy) dict. |

**Returns:** An ``ExistsResult`` NamedTuple with ``key``, ``meta`` fields.
    ``meta`` is ``None`` if the record does not exist.

##### Sync Client

```python
result = client.exists(("test", "demo", "user1"))
if result.meta is not None:
    print(f"Found, gen={result.meta.gen}")
```

##### Async Client

```python
result = await client.exists(("test", "demo", "user1"))
if result.meta is not None:
    print(f"Found, gen={result.meta.gen}")
```

##### `remove(key, meta=None, policy=None)`

Delete a record from the cluster.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict for generation check. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

> **Note:**
> Raises `RecordNotFound` The record does not exist.
##### Sync Client

```python
client.remove(("test", "demo", "user1"))

# With generation check
import aerospike_py
client.remove(
    key,
    meta={"gen": 3},
    policy={"gen": aerospike_py.POLICY_GEN_EQ},
)
```

##### Async Client

```python
await client.remove(("test", "demo", "user1"))
```

##### `touch(key, val=0, meta=None, policy=None)`

Reset the TTL of a record.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `val` | New TTL value in seconds. |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

> **Note:**
> Raises `RecordNotFound` The record does not exist.
##### Sync Client

```python
client.touch(("test", "demo", "user1"), val=300)
```

##### Async Client

```python
await client.touch(("test", "demo", "user1"), val=300)
```

#### String / Numeric Operations

##### `append(key, bin, val, meta=None, policy=None)`

Append a string to a bin value.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `bin` | Target bin name. |
| `val` | String value to append. |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

##### Sync Client

```python
client.append(("test", "demo", "user1"), "name", "_suffix")
```

##### Async Client

```python
await client.append(("test", "demo", "user1"), "name", "_suffix")
```

##### `prepend(key, bin, val, meta=None, policy=None)`

Prepend a string to a bin value.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `bin` | Target bin name. |
| `val` | String value to prepend. |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

##### Sync Client

```python
client.prepend(("test", "demo", "user1"), "name", "prefix_")
```

##### Async Client

```python
await client.prepend(("test", "demo", "user1"), "name", "prefix_")
```

##### `increment(key, bin, offset, meta=None, policy=None)`

Increment a numeric bin value.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `bin` | Target bin name. |
| `offset` | Integer or float amount to add (use negative to decrement). |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

##### Sync Client

```python
client.increment(("test", "demo", "user1"), "age", 1)
client.increment(("test", "demo", "user1"), "score", 0.5)
```

##### Async Client

```python
await client.increment(("test", "demo", "user1"), "age", 1)
await client.increment(("test", "demo", "user1"), "score", 0.5)
```

##### `remove_bin(key, bin_names, meta=None, policy=None)`

Remove specific bins from a record by setting them to nil.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `bin_names` | List of bin names to remove. |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

##### Sync Client

```python
client.remove_bin(("test", "demo", "user1"), ["temp_bin", "debug_bin"])
```

##### Async Client

```python
await client.remove_bin(("test", "demo", "user1"), ["temp_bin"])
```

#### Multi-Operation

##### `operate(key, ops, meta=None, policy=None)`

Execute multiple operations atomically on a single record.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `ops` | List of operation dicts with ``"op"``, ``"bin"``, ``"val"`` keys. |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

**Returns:** A ``Record`` NamedTuple with ``key``, ``meta``, ``bins`` fields.

##### Sync Client

```python
import aerospike_py

ops = [
    {"op": aerospike_py.OPERATOR_INCR, "bin": "counter", "val": 1},
    {"op": aerospike_py.OPERATOR_READ, "bin": "counter", "val": None},
]
record = client.operate(("test", "demo", "key1"), ops)
print(record.bins)
```

##### Async Client

```python
import aerospike_py

ops = [
    {"op": aerospike_py.OPERATOR_INCR, "bin": "counter", "val": 1},
    {"op": aerospike_py.OPERATOR_READ, "bin": "counter", "val": None},
]
record = await client.operate(("test", "demo", "key1"), ops)
print(record.bins)
```

##### `operate_ordered(key, ops, meta=None, policy=None)`

Execute multiple operations with ordered results.

Like ``operate()`` but returns results as an ordered list preserving
the operation order.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `ops` | List of operation dicts with ``"op"``, ``"bin"``, ``"val"`` keys. |
| `meta` | Optional [`WriteMeta`](types.md#writemeta) dict. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

**Returns:** An ``OperateOrderedResult`` NamedTuple with ``key``, ``meta``,
    ``ordered_bins`` fields.

##### Sync Client

```python
import aerospike_py

ops = [
    {"op": aerospike_py.OPERATOR_INCR, "bin": "counter", "val": 1},
    {"op": aerospike_py.OPERATOR_READ, "bin": "counter", "val": None},
]
result = client.operate_ordered(("test", "demo", "key1"), ops)
# result.ordered_bins = [BinTuple("counter", 2)]
```

##### Async Client

```python
import aerospike_py

ops = [
    {"op": aerospike_py.OPERATOR_INCR, "bin": "counter", "val": 1},
    {"op": aerospike_py.OPERATOR_READ, "bin": "counter", "val": None},
]
result = await client.operate_ordered(
    ("test", "demo", "key1"), ops
)
# result.ordered_bins = [BinTuple("counter", 2)]
```

#### Batch Operations

##### `batch_read(keys, bins=None, policy=None, _dtype=None)`

Read multiple records in a single batch call.

| Parameter | Description |
|-----------|-------------|
| `keys` | List of ``(namespace, set, primary_key)`` tuples. |
| `bins` | Optional list of bin names to read. ``None`` reads all bins; an empty list performs an existence check only. |
| `policy` | Optional [`BatchPolicy`](types.md#batchpolicy) dict. |
| `_dtype` | Optional NumPy dtype. When provided, returns ``NumpyBatchRecords`` instead of ``BatchRecords``. |

**Returns:** ``BatchRecords`` (or ``NumpyBatchRecords`` when ``_dtype`` is set).

##### Sync Client

```python
keys = [("test", "demo", f"user_{i}") for i in range(10)]

batch = client.batch_read(keys)
for br in batch.batch_records:
    if br.record:
        print(br.record.bins)

# Read specific bins
batch = client.batch_read(keys, bins=["name", "age"])
```

##### Async Client

```python
keys = [("test", "demo", f"user_{i}") for i in range(10)]
batch = await client.batch_read(keys, bins=["name", "age"])
for br in batch.batch_records:
    if br.record:
        print(br.record.bins)
```

##### `batch_operate(keys, ops, policy=None)`

Execute operations on multiple records in a single batch call.

| Parameter | Description |
|-----------|-------------|
| `keys` | List of ``(namespace, set, primary_key)`` tuples. |
| `ops` | List of operation dicts to apply to each record. |
| `policy` | Optional [`BatchPolicy`](types.md#batchpolicy) dict. |

**Returns:** A list of ``Record`` NamedTuples.

##### Sync Client

```python
import aerospike_py

keys = [("test", "demo", f"user_{i}") for i in range(10)]
ops = [{"op": aerospike_py.OPERATOR_INCR, "bin": "views", "val": 1}]
results = client.batch_operate(keys, ops)
```

##### Async Client

```python
import aerospike_py

keys = [("test", "demo", f"user_{i}") for i in range(10)]
ops = [{"op": aerospike_py.OPERATOR_INCR, "bin": "views", "val": 1}]
results = await client.batch_operate(keys, ops)
```

##### `batch_remove(keys, policy=None)`

Delete multiple records in a single batch call.

| Parameter | Description |
|-----------|-------------|
| `keys` | List of ``(namespace, set, primary_key)`` tuples. |
| `policy` | Optional [`BatchPolicy`](types.md#batchpolicy) dict. |

**Returns:** A list of ``Record`` NamedTuples.

##### Sync Client

```python
keys = [("test", "demo", f"user_{i}") for i in range(10)]
results = client.batch_remove(keys)
```

##### Async Client

```python
keys = [("test", "demo", f"user_{i}") for i in range(10)]
results = await client.batch_remove(keys)
```

#### Query & Scan

##### `query(namespace, set_name)`

Create a Query object for secondary index queries.

| Parameter | Description |
|-----------|-------------|
| `namespace` | The namespace to query. |
| `set_name` | The set to query. |

**Returns:** A ``Query`` object. Use ``where()`` to set a predicate filter
    and ``results()`` or ``foreach()`` to execute.

##### Sync Client

```python
query = client.query("test", "demo")
query.select("name", "age")
query.where(predicates.between("age", 20, 30))
records = query.results()
```

##### Async Client

```python
query = client.query("test", "demo")
query.where(predicates.between("age", 20, 30))
records = await query.results()
```

#### Index Management

##### `index_integer_create(namespace, set_name, bin_name, index_name, policy=None)`

Create a numeric secondary index.

| Parameter | Description |
|-----------|-------------|
| `namespace` | Target namespace. |
| `set_name` | Target set. |
| `bin_name` | Bin to index. |
| `index_name` | Name for the new index. |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

> **Note:**
> Raises `IndexFoundError` An index with that name already exists.
##### Sync Client

```python
client.index_integer_create("test", "demo", "age", "age_idx")
```

##### Async Client

```python
await client.index_integer_create("test", "demo", "age", "age_idx")
```

##### `index_string_create(namespace, set_name, bin_name, index_name, policy=None)`

Create a string secondary index.

| Parameter | Description |
|-----------|-------------|
| `namespace` | Target namespace. |
| `set_name` | Target set. |
| `bin_name` | Bin to index. |
| `index_name` | Name for the new index. |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

> **Note:**
> Raises `IndexFoundError` An index with that name already exists.
##### Sync Client

```python
client.index_string_create("test", "demo", "name", "name_idx")
```

##### Async Client

```python
await client.index_string_create("test", "demo", "name", "name_idx")
```

##### `index_geo2dsphere_create(namespace, set_name, bin_name, index_name, policy=None)`

Create a geospatial secondary index.

| Parameter | Description |
|-----------|-------------|
| `namespace` | Target namespace. |
| `set_name` | Target set. |
| `bin_name` | Bin to index (must contain GeoJSON values). |
| `index_name` | Name for the new index. |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

> **Note:**
> Raises `IndexFoundError` An index with that name already exists.
##### Sync Client

```python
client.index_geo2dsphere_create("test", "demo", "location", "geo_idx")
```

##### Async Client

```python
await client.index_geo2dsphere_create(
    "test", "demo", "location", "geo_idx"
)
```

##### `index_remove(namespace, index_name, policy=None)`

Remove a secondary index.

| Parameter | Description |
|-----------|-------------|
| `namespace` | Target namespace. |
| `index_name` | Name of the index to remove. |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

> **Note:**
> Raises `IndexNotFound` The index does not exist.
##### Sync Client

```python
client.index_remove("test", "age_idx")
```

##### Async Client

```python
await client.index_remove("test", "age_idx")
```

#### Truncate

##### `truncate(namespace, set_name, nanos=0, policy=None)`

Remove all records in a namespace/set.

| Parameter | Description |
|-----------|-------------|
| `namespace` | Target namespace. |
| `set_name` | Target set. |
| `nanos` | Optional last-update cutoff in nanoseconds. |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

##### Sync Client

```python
client.truncate("test", "demo")
```

##### Async Client

```python
await client.truncate("test", "demo")
```

#### UDF

##### `udf_put(filename, udf_type=0, policy=None)`

Register a Lua UDF module on the cluster.

| Parameter | Description |
|-----------|-------------|
| `filename` | Path to the Lua source file. |
| `udf_type` | UDF language type (only Lua ``0`` is supported). |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

##### Sync Client

```python
client.udf_put("my_udf.lua")
```

##### Async Client

```python
await client.udf_put("my_udf.lua")
```

##### `udf_remove(module, policy=None)`

Remove a registered UDF module.

| Parameter | Description |
|-----------|-------------|
| `module` | Module name to remove (without ``.lua`` extension). |
| `policy` | Optional [`AdminPolicy`](types.md#adminpolicy) dict. |

##### Sync Client

```python
client.udf_remove("my_udf")
```

##### Async Client

```python
await client.udf_remove("my_udf")
```

##### `apply(key, module, function, args=None, policy=None)`

Execute a UDF on a single record.

| Parameter | Description |
|-----------|-------------|
| `key` | Record key as ``(namespace, set, primary_key)`` tuple. |
| `module` | Name of the registered UDF module. |
| `function` | Name of the function within the module. |
| `args` | Optional list of arguments to pass to the function. |
| `policy` | Optional [`WritePolicy`](types.md#writepolicy) dict. |

**Returns:** The return value of the UDF function.

##### Sync Client

```python
result = client.apply(
    ("test", "demo", "key1"),
    "my_udf",
    "my_function",
    [1, "hello"],
)
```

##### Async Client

```python
result = await client.apply(
    ("test", "demo", "key1"),
    "my_udf",
    "my_function",
    [1, "hello"],
)
```

#### Query Object

Secondary index query object.

Created via ``Client.query(namespace, set_name)``. Use ``where()``
to set a predicate filter, ``select()`` to choose bins, then
``results()`` or ``foreach()`` to execute.

```python
from aerospike_py import predicates

query = client.query("test", "demo")
query.select("name", "age")
query.where(predicates.between("age", 20, 30))
records = query.results()
```

##### `select()`

Select specific bins to return in query results.

```python
query = client.query("test", "demo")
query.select("name", "age")
```

##### `where(predicate)`

Set a predicate filter for the query.

Requires a matching secondary index on the filtered bin.

| Parameter | Description |
|-----------|-------------|
| `predicate` | A predicate tuple created by ``aerospike_py.predicates`` helper functions. |

```python
from aerospike_py import predicates

query = client.query("test", "demo")
query.where(predicates.equals("name", "Alice"))
```

##### `results(policy=None)`

Execute the query and return all matching records.

| Parameter | Description |
|-----------|-------------|
| `policy` | Optional [`QueryPolicy`](types.md#querypolicy) dict. |

**Returns:** A list of ``Record`` NamedTuples.

```python
records = query.results()
for record in records:
    print(record.bins)
```

##### `foreach(callback, policy=None)`

Execute the query and invoke a callback for each record.

The callback receives a ``Record`` NamedTuple. Return ``False``
from the callback to stop iteration early.

| Parameter | Description |
|-----------|-------------|
| `callback` | Function called with each record. Return ``False`` to stop. |
| `policy` | Optional [`QueryPolicy`](types.md#querypolicy) dict. |

```python
def process(record):
    print(record.bins)

query.foreach(process)
```


---

### Constants

> All constants used across the aerospike-py API.

```python
import aerospike_py as aerospike
```

#### Policy

##### Key

| Constant | Value | Description |
|----------|-------|-------------|
| `POLICY_KEY_DIGEST` | 0 | Store only the digest (default) |
| `POLICY_KEY_SEND` | 1 | Send and store the key |

##### Exists

| Constant | Value | Description |
|----------|-------|-------------|
| `POLICY_EXISTS_IGNORE` | 0 | Write regardless (default) |
| `POLICY_EXISTS_UPDATE` | 1 | Update existing |
| `POLICY_EXISTS_UPDATE_ONLY` | 2 | Fail if not exists |
| `POLICY_EXISTS_REPLACE` | 3 | Replace all bins |
| `POLICY_EXISTS_REPLACE_ONLY` | 4 | Replace only if exists |
| `POLICY_EXISTS_CREATE_ONLY` | 5 | Fail if exists |

##### Generation

| Constant | Value | Description |
|----------|-------|-------------|
| `POLICY_GEN_IGNORE` | 0 | Ignore generation (default) |
| `POLICY_GEN_EQ` | 1 | Write only if gen matches |
| `POLICY_GEN_GT` | 2 | Write only if gen is greater |

##### Replica

| Constant | Value | Description |
|----------|-------|-------------|
| `POLICY_REPLICA_MASTER` | 0 | Read from master |
| `POLICY_REPLICA_SEQUENCE` | 1 | Round-robin (default) |
| `POLICY_REPLICA_PREFER_RACK` | 2 | Prefer rack-local |

##### Commit Level

| Constant | Value | Description |
|----------|-------|-------------|
| `POLICY_COMMIT_LEVEL_ALL` | 0 | Wait for all replicas |
| `POLICY_COMMIT_LEVEL_MASTER` | 1 | Master only |

##### Read Mode AP

| Constant | Value | Description |
|----------|-------|-------------|
| `POLICY_READ_MODE_AP_ONE` | 0 | Read from one node |
| `POLICY_READ_MODE_AP_ALL` | 1 | Read from all nodes |

#### TTL

| Constant | Value | Description |
|----------|-------|-------------|
| `TTL_NAMESPACE_DEFAULT` | 0 | Use namespace default |
| `TTL_NEVER_EXPIRE` | -1 | Never expire |
| `TTL_DONT_UPDATE` | -2 | Don't update TTL on write |
| `TTL_CLIENT_DEFAULT` | -3 | Use client default |

#### Auth Mode

| Constant | Value | Description |
|----------|-------|-------------|
| `AUTH_INTERNAL` | 0 | Internal authentication |
| `AUTH_EXTERNAL` | 1 | External (LDAP) |
| `AUTH_PKI` | 2 | PKI authentication |

#### Operators

Used with `operate()` and `batch_operate()`.

| Constant | Value | Description |
|----------|-------|-------------|
| `OPERATOR_READ` | 1 | Read a bin |
| `OPERATOR_WRITE` | 2 | Write a bin |
| `OPERATOR_INCR` | 5 | Increment int/float bin |
| `OPERATOR_APPEND` | 9 | Append to string bin |
| `OPERATOR_PREPEND` | 10 | Prepend to string bin |
| `OPERATOR_TOUCH` | 11 | Reset record TTL |
| `OPERATOR_DELETE` | 14 | Delete the record |

#### Index Type

| Constant | Value | Description |
|----------|-------|-------------|
| `INDEX_NUMERIC` | 0 | Numeric |
| `INDEX_STRING` | 1 | String |
| `INDEX_BLOB` | 2 | Blob |
| `INDEX_GEO2DSPHERE` | 3 | Geospatial |

#### Index Collection Type

| Constant | Value | Description |
|----------|-------|-------------|
| `INDEX_TYPE_DEFAULT` | 0 | Scalar (default) |
| `INDEX_TYPE_LIST` | 1 | List elements |
| `INDEX_TYPE_MAPKEYS` | 2 | Map keys |
| `INDEX_TYPE_MAPVALUES` | 3 | Map values |

#### Log Level

| Constant | Value | Description |
|----------|-------|-------------|
| `LOG_LEVEL_OFF` | -1 | Disabled |
| `LOG_LEVEL_ERROR` | 0 | Error only |
| `LOG_LEVEL_WARN` | 1 | Warnings+ |
| `LOG_LEVEL_INFO` | 2 | Info+ |
| `LOG_LEVEL_DEBUG` | 3 | Debug+ |
| `LOG_LEVEL_TRACE` | 4 | All |

#### Serializer

| Constant | Value | Description |
|----------|-------|-------------|
| `SERIALIZER_NONE` | 0 | No serialization |
| `SERIALIZER_PYTHON` | 1 | Python pickle |
| `SERIALIZER_USER` | 2 | User-defined |

#### List CDT

##### Return Type

| Constant | Description |
|----------|-------------|
| `LIST_RETURN_NONE` | No return |
| `LIST_RETURN_INDEX` | Index |
| `LIST_RETURN_REVERSE_INDEX` | Reverse index |
| `LIST_RETURN_RANK` | Rank |
| `LIST_RETURN_REVERSE_RANK` | Reverse rank |
| `LIST_RETURN_COUNT` | Count |
| `LIST_RETURN_VALUE` | Value |
| `LIST_RETURN_EXISTS` | Boolean |

##### Order

| Constant | Description |
|----------|-------------|
| `LIST_UNORDERED` | Unordered (default) |
| `LIST_ORDERED` | Ordered |

##### Sort Flags

| Constant | Description |
|----------|-------------|
| `LIST_SORT_DEFAULT` | Default sort |
| `LIST_SORT_DROP_DUPLICATES` | Drop duplicates |

##### Write Flags

| Constant | Description |
|----------|-------------|
| `LIST_WRITE_DEFAULT` | Default |
| `LIST_WRITE_ADD_UNIQUE` | Unique values only |
| `LIST_WRITE_INSERT_BOUNDED` | Enforce boundaries |
| `LIST_WRITE_NO_FAIL` | No-fail on violation |
| `LIST_WRITE_PARTIAL` | Allow partial success |

#### Map CDT

##### Return Type

| Constant | Description |
|----------|-------------|
| `MAP_RETURN_NONE` | No return |
| `MAP_RETURN_INDEX` | Index |
| `MAP_RETURN_REVERSE_INDEX` | Reverse index |
| `MAP_RETURN_RANK` | Rank |
| `MAP_RETURN_REVERSE_RANK` | Reverse rank |
| `MAP_RETURN_COUNT` | Count |
| `MAP_RETURN_KEY` | Key |
| `MAP_RETURN_VALUE` | Value |
| `MAP_RETURN_KEY_VALUE` | Key-value pair |
| `MAP_RETURN_EXISTS` | Boolean |

##### Order

| Constant | Description |
|----------|-------------|
| `MAP_UNORDERED` | Unordered (default) |
| `MAP_KEY_ORDERED` | Key-ordered |
| `MAP_KEY_VALUE_ORDERED` | Key-value ordered |

##### Write Flags

| Constant | Description |
|----------|-------------|
| `MAP_WRITE_FLAGS_DEFAULT` | Default |
| `MAP_WRITE_FLAGS_CREATE_ONLY` | Create only |
| `MAP_WRITE_FLAGS_UPDATE_ONLY` | Update only |
| `MAP_WRITE_FLAGS_NO_FAIL` | No-fail |
| `MAP_WRITE_FLAGS_PARTIAL` | Partial success |
| `MAP_UPDATE` | Update map |
| `MAP_UPDATE_ONLY` | Update existing only |
| `MAP_CREATE_ONLY` | Create new only |

#### Bit / HLL Write Flags

| Constant | Description |
|----------|-------------|
| `BIT_WRITE_DEFAULT` | Default |
| `BIT_WRITE_CREATE_ONLY` | Create only |
| `BIT_WRITE_UPDATE_ONLY` | Update only |
| `BIT_WRITE_NO_FAIL` | No-fail |
| `BIT_WRITE_PARTIAL` | Partial |
| `HLL_WRITE_DEFAULT` | Default |
| `HLL_WRITE_CREATE_ONLY` | Create only |
| `HLL_WRITE_UPDATE_ONLY` | Update only |
| `HLL_WRITE_NO_FAIL` | No-fail |
| `HLL_WRITE_ALLOW_FOLD` | Allow fold |

#### Privilege Codes

| Constant | Description |
|----------|-------------|
| `PRIV_READ` | Read |
| `PRIV_WRITE` | Write |
| `PRIV_READ_WRITE` | Read-write |
| `PRIV_READ_WRITE_UDF` | Read-write-UDF |
| `PRIV_SYS_ADMIN` | System admin |
| `PRIV_USER_ADMIN` | User admin |
| `PRIV_DATA_ADMIN` | Data admin |
| `PRIV_UDF_ADMIN` | UDF admin |
| `PRIV_SINDEX_ADMIN` | Secondary index admin |
| `PRIV_TRUNCATE` | Truncate |

#### Status Codes

| Constant | Description |
|----------|-------------|
| `AEROSPIKE_OK` | Success |
| `AEROSPIKE_ERR_SERVER` | Server error |
| `AEROSPIKE_ERR_RECORD_NOT_FOUND` | Record not found |
| `AEROSPIKE_ERR_RECORD_GENERATION` | Generation mismatch |
| `AEROSPIKE_ERR_PARAM` | Invalid parameter |
| `AEROSPIKE_ERR_RECORD_EXISTS` | Record exists |
| `AEROSPIKE_ERR_BIN_EXISTS` | Bin exists |
| `AEROSPIKE_ERR_TIMEOUT` | Timeout |
| `AEROSPIKE_ERR_BIN_TYPE` | Bin type mismatch |
| `AEROSPIKE_ERR_RECORD_TOO_BIG` | Record too big |
| `AEROSPIKE_ERR_BIN_NOT_FOUND` | Bin not found |
| `AEROSPIKE_ERR_INVALID_NAMESPACE` | Invalid namespace |
| `AEROSPIKE_ERR_BIN_NAME` | Invalid bin name |
| `AEROSPIKE_ERR_FILTERED_OUT` | Filtered out |
| `AEROSPIKE_ERR_UDF` | UDF error |
| `AEROSPIKE_ERR_INDEX_FOUND` | Index exists |
| `AEROSPIKE_ERR_INDEX_NOT_FOUND` | Index not found |
| `AEROSPIKE_ERR_QUERY_ABORTED` | Query aborted |
| `AEROSPIKE_ERR_CLIENT` | Client error |
| `AEROSPIKE_ERR_CONNECTION` | Connection error |
| `AEROSPIKE_ERR_CLUSTER` | Cluster error |
| `AEROSPIKE_ERR_INVALID_HOST` | Invalid host |
| `AEROSPIKE_ERR_NO_MORE_CONNECTIONS` | No connections |

See `__init__.pyi` for the complete list.


---

### Exceptions

> Exception hierarchy and error handling patterns.

```python
from aerospike_py.exception import RecordNotFound, AerospikeError
```

#### Hierarchy

```
Exception
└── AerospikeError
    ├── ClientError
    ├── ClusterError
    ├── InvalidArgError
    ├── AerospikeTimeoutError
    ├── ServerError
    │   ├── AerospikeIndexError
    │   │   ├── IndexNotFound
    │   │   └── IndexFoundError
    │   ├── QueryError
    │   │   └── QueryAbortedError
    │   ├── AdminError
    │   └── UDFError
    └── RecordError
        ├── RecordNotFound
        ├── RecordExistsError
        ├── RecordGenerationError
        ├── RecordTooBig
        ├── BinNameError
        ├── BinExistsError
        ├── BinNotFound
        ├── BinTypeError
        └── FilteredOut
```

#### Reference

##### Base

| Exception | Description |
|-----------|-------------|
| `AerospikeError` | Base for all Aerospike exceptions |
| `ClientError` | Client-side errors (connection, config) |
| `ClusterError` | Cluster connection/discovery errors |
| `InvalidArgError` | Invalid argument |
| `AerospikeTimeoutError` | Operation timed out |
| `ServerError` | Server-side errors |
| `RecordError` | Record-level errors |

##### Record

| Exception | Description |
|-----------|-------------|
| `RecordNotFound` | Record does not exist |
| `RecordExistsError` | Record already exists (`CREATE_ONLY`) |
| `RecordGenerationError` | Generation mismatch (optimistic lock) |
| `RecordTooBig` | Record exceeds size limit |
| `BinNameError` | Invalid bin name |
| `BinExistsError` | Bin already exists |
| `BinNotFound` | Bin does not exist |
| `BinTypeError` | Bin type mismatch |
| `FilteredOut` | Excluded by expression filter |

##### Server

| Exception | Description |
|-----------|-------------|
| `AerospikeIndexError` | Secondary index error |
| `IndexNotFound` | Index does not exist |
| `IndexFoundError` | Index already exists |
| `QueryError` | Query execution error |
| `QueryAbortedError` | Query aborted |
| `AdminError` | Admin operation error |
| `UDFError` | UDF error |

> **Note:**
> `TimeoutError` and `IndexError` are deprecated aliases for `AerospikeTimeoutError` and `AerospikeIndexError` to avoid shadowing Python builtins.
#### Examples

```python
from aerospike_py.exception import (
    RecordNotFound,
    RecordExistsError,
    RecordGenerationError,
    AerospikeTimeoutError,
    AerospikeError,
)

# Basic error handling
try:
    record = client.get(("test", "demo", "nonexistent"))
except RecordNotFound:
    print("Not found")
except AerospikeError as e:
    print(f"Error: {e}")

# Optimistic locking
try:
    record = client.get(key)
    client.put(
        key,
        {"val": record.bins["val"] + 1},
        meta={"gen": record.meta.gen},
        policy={"gen": aerospike.POLICY_GEN_EQ},
    )
except RecordGenerationError:
    print("Concurrent modification detected")

# Create-only
try:
    client.put(key, bins, policy={"exists": aerospike.POLICY_EXISTS_CREATE_ONLY})
except RecordExistsError:
    print("Already exists")
```

See the [Error Handling Guide](/docs/guides/admin/error-handling) for production patterns.


---

### Query API

> Query and AsyncQuery class reference with predicates.

#### Query / AsyncQuery

Created via `client.query(namespace, set_name)`. Use `where()` to set a predicate, `select()` to choose bins, then `results()` or `foreach()` to execute.

```python
from aerospike_py import predicates

query = client.query("test", "demo")
query.select("name", "age")
query.where(predicates.between("age", 20, 30))
records = query.results()  # or: await query.results()
```

##### `select(*bins)`

Select specific bins to return.

##### `where(predicate)`

Set a predicate filter. Requires a secondary index on the bin.

##### `results(policy=None) -> list[Record]`

Execute and return all matching records.

##### `foreach(callback, policy=None)`

Execute and invoke `callback(record)` for each result. Return `False` to stop early.

```python
def process(record: Record) -> None:
    print(record.bins)

query.foreach(process)
```

---

#### Predicates

```python
from aerospike_py import predicates
```

| Function | Description | Example |
|----------|-------------|---------|
| `equals(bin, val)` | Equality | `equals("name", "Alice")` |
| `between(bin, min, max)` | Inclusive range | `between("age", 20, 30)` |
| `contains(bin, idx_type, val)` | List/map contains | `contains("tags", INDEX_TYPE_LIST, "py")` |
| `geo_within_geojson_region(bin, geojson)` | Points in region | See below |
| `geo_within_radius(bin, lat, lng, radius)` | Points in circle (meters) | See below |
| `geo_contains_geojson_point(bin, geojson)` | Regions containing point | See below |

##### Geospatial

```python
# Points within a polygon
region = '{"type":"Polygon","coordinates":[[[126.9,37.5],[126.9,37.6],[127.0,37.6],[127.0,37.5],[126.9,37.5]]]}'
query.where(predicates.geo_within_geojson_region("location", region))

# Points within radius
query.where(predicates.geo_within_radius("location", 37.5665, 126.978, 5000.0))

# Regions containing a point
point = '{"type":"Point","coordinates":[126.978, 37.5665]}'
query.where(predicates.geo_contains_geojson_point("coverage", point))
```

---

#### Full Example

```python
import aerospike_py as aerospike
from aerospike_py import predicates, Record

client = aerospike.client({"hosts": [("127.0.0.1", 3000)]}).connect()

# Insert data
for i in range(100):
    client.put(("test", "users", f"user_{i}"), {
        "name": f"User {i}",
        "age": 20 + (i % 40),
    })

# Create index
client.index_integer_create("test", "users", "age", "users_age_idx")

# Query
query = client.query("test", "users")
query.select("name", "age")
query.where(predicates.between("age", 25, 35))
records: list[Record] = query.results()

for record in records:
    print(f"{record.bins['name']}: age {record.bins['age']}")

# Cleanup
client.index_remove("test", "users_age_idx")
client.close()
```


---

### Types

> NamedTuple return types and TypedDict input types.

All types are importable from `aerospike_py` or `aerospike_py.types`.

```python
from aerospike_py import Record, ExistsResult, ReadPolicy, WritePolicy, WriteMeta
```

#### Return Types (NamedTuple)

All return types support both attribute access and tuple unpacking.

##### `Record`

Returned by: `get()`, `select()`, `operate()`, `batch_operate()`, `batch_remove()`, `Query.results()`

| Field | Type | Description |
|-------|------|-------------|
| `key` | `AerospikeKey \| None` | Record key |
| `meta` | `RecordMetadata \| None` | Generation and TTL |
| `bins` | `dict[str, Any] \| None` | Bin values |

```python
record: Record = client.get(key)
print(record.bins)         # attribute access
_, meta, bins = record     # tuple unpacking
```

##### `RecordMetadata`

| Field | Type | Description |
|-------|------|-------------|
| `gen` | `int` | Generation (optimistic lock version) |
| `ttl` | `int` | Time-to-live in seconds |

##### `AerospikeKey`

| Field | Type | Description |
|-------|------|-------------|
| `namespace` | `str` | Namespace |
| `set_name` | `str` | Set name |
| `user_key` | `str \| int \| bytes \| None` | Primary key (`None` if `POLICY_KEY_DIGEST`) |
| `digest` | `bytes` | 20-byte RIPEMD-160 digest |

##### `ExistsResult`

Returned by: `exists()`

| Field | Type | Description |
|-------|------|-------------|
| `key` | `AerospikeKey \| None` | Record key |
| `meta` | `RecordMetadata \| None` | `None` if record does not exist |

```python
result: ExistsResult = client.exists(key)
if result.meta is not None:
    print(f"gen={result.meta.gen}")
```

##### `InfoNodeResult`

Returned by: `info_all()`

| Field | Type | Description |
|-------|------|-------------|
| `node_name` | `str` | Cluster node name |
| `error_code` | `int` | 0 on success |
| `response` | `str` | Info response string |

##### `OperateOrderedResult`

Returned by: `operate_ordered()`

| Field | Type | Description |
|-------|------|-------------|
| `key` | `AerospikeKey \| None` | Record key |
| `meta` | `RecordMetadata \| None` | Record metadata |
| `ordered_bins` | `list[BinTuple]` | Ordered operation results |

##### `BinTuple`

| Field | Type | Description |
|-------|------|-------------|
| `name` | `str` | Bin name |
| `value` | `Any` | Bin value |

##### Return Type Quick Reference

| Method | Return Type |
|--------|-------------|
| `get()`, `select()` | `Record` |
| `exists()` | `ExistsResult` |
| `operate()` | `Record` |
| `operate_ordered()` | `OperateOrderedResult` |
| `info_all()` | `list[InfoNodeResult]` |
| `batch_read()` | `BatchRecords` \| `NumpyBatchRecords` |
| `batch_operate()`, `batch_remove()` | `list[Record]` |
| `Query.results()` | `list[Record]` |

---

#### Input Types (TypedDict)

All fields are optional (`total=False`).

##### `ClientConfig`

Used by: `aerospike_py.client(config)`, `AsyncClient(config)`

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `hosts` | `list[tuple[str, int]]` | *required* | Seed nodes |
| `cluster_name` | `str` | | Expected cluster name |
| `auth_mode` | `int` | `AUTH_INTERNAL` | `AUTH_INTERNAL`, `AUTH_EXTERNAL`, `AUTH_PKI` |
| `user` | `str` | | Authentication username |
| `password` | `str` | | Authentication password |
| `timeout` | `int` | `1000` | Connection timeout (ms) |
| `idle_timeout` | `int` | | Connection idle timeout (ms) |
| `max_conns_per_node` | `int` | `100` | Max connections per node |
| `min_conns_per_node` | `int` | `0` | Pre-warm connections |
| `tend_interval` | `int` | `1000` | Cluster tend interval (ms) |
| `use_services_alternate` | `bool` | `false` | Use alternate service addresses |

##### `ReadPolicy`

Used by: `get()`, `select()`, `exists()`

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `socket_timeout` | `int` | `30000` | Socket timeout (ms) |
| `total_timeout` | `int` | `1000` | Total transaction timeout (ms) |
| `max_retries` | `int` | `2` | Max retries |
| `sleep_between_retries` | `int` | `0` | Sleep between retries (ms) |
| `expressions` | `Any` | | Expression filter (`aerospike_py.exp`) |
| `replica` | `int` | `POLICY_REPLICA_SEQUENCE` | Replica algorithm |
| `read_mode_ap` | `int` | `POLICY_READ_MODE_AP_ONE` | AP read consistency |

##### `WritePolicy`

Used by: `put()`, `remove()`, `touch()`, `append()`, `prepend()`, `increment()`, `remove_bin()`, `operate()`, `operate_ordered()`

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `socket_timeout` | `int` | `30000` | Socket timeout (ms) |
| `total_timeout` | `int` | `1000` | Total transaction timeout (ms) |
| `max_retries` | `int` | `0` | Max retries |
| `durable_delete` | `bool` | `false` | Durable delete (Enterprise) |
| `key` | `int` | `POLICY_KEY_DIGEST` | Key send policy |
| `exists` | `int` | `POLICY_EXISTS_IGNORE` | Existence policy |
| `gen` | `int` | `POLICY_GEN_IGNORE` | Generation policy |
| `commit_level` | `int` | `POLICY_COMMIT_LEVEL_ALL` | Commit level |
| `ttl` | `int` | `0` | Record TTL (seconds) |
| `expressions` | `Any` | | Expression filter |

##### `BatchPolicy`

Used by: `batch_read()`, `batch_operate()`, `batch_remove()`

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `socket_timeout` | `int` | `30000` | Socket timeout (ms) |
| `total_timeout` | `int` | `1000` | Total transaction timeout (ms) |
| `max_retries` | `int` | `2` | Max retries |
| `filter_expression` | `Any` | | Expression filter |

##### `QueryPolicy`

Used by: `Query.results()`, `Query.foreach()`

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `socket_timeout` | `int` | `30000` | Socket timeout (ms) |
| `total_timeout` | `int` | `0` | Total timeout (0 = no limit) |
| `max_retries` | `int` | `2` | Max retries |
| `max_records` | `int` | `0` | Max records (0 = all) |
| `records_per_second` | `int` | `0` | Rate limit (0 = unlimited) |
| `expressions` | `Any` | | Expression filter |

##### `AdminPolicy`

Used by: all `admin_*` methods, index operations, `truncate()`

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `timeout` | `int` | `1000` | Timeout (ms) |

##### `WriteMeta`

Used by: `put()`, `remove()`, `touch()`, `operate()` etc. as `meta` parameter

| Field | Type | Description |
|-------|------|-------------|
| `gen` | `int` | Expected generation (for `POLICY_GEN_EQ`) |
| `ttl` | `int` | Record TTL in seconds |

##### `Privilege`

Used by: `admin_create_role()`, `admin_grant_privileges()`, `admin_revoke_privileges()`

| Field | Type | Description |
|-------|------|-------------|
| `code` | `int` | Privilege code (`PRIV_READ`, `PRIV_WRITE`, etc.) |
| `ns` | `str` | Namespace scope (empty = global) |
| `set` | `str` | Set scope (empty = namespace-wide) |

##### `UserInfo`

Returned by: `admin_query_user_info()`, `admin_query_users_info()`

| Field | Type | Description |
|-------|------|-------------|
| `user` | `str` | Username |
| `roles` | `list[str]` | Assigned roles |
| `conns_in_use` | `int` | Active connections |

##### `RoleInfo`

Returned by: `admin_query_role()`, `admin_query_roles()`

| Field | Type | Description |
|-------|------|-------------|
| `name` | `str` | Role name |
| `privileges` | `list[Privilege]` | Assigned privileges |
| `allowlist` | `list[str]` | IP allowlist |
| `read_quota` | `int` | Read quota |
| `write_quota` | `int` | Write quota |


---

## Guides

### Admin Guide

> User and role management for security-enabled Aerospike clusters.

Requires a security-enabled Aerospike server.

#### User Management

```python
import aerospike_py as aerospike

# Create user
client.admin_create_user("alice", "secure_password", ["read-write"])

# Change password
client.admin_change_password("alice", "new_password")

# Grant / revoke roles
client.admin_grant_roles("alice", ["sys-admin"])
client.admin_revoke_roles("alice", ["read-write"])

# Query users
user = client.admin_query_user_info("alice")
users = client.admin_query_users_info()

# Drop user
client.admin_drop_user("alice")
```

#### Role Management

```python
# Create role with namespace/set-scoped privileges
client.admin_create_role("data_reader", [
    {"code": aerospike.PRIV_READ, "ns": "test", "set": "demo"},
])

# Create role with global privileges
client.admin_create_role("full_admin", [
    {"code": aerospike.PRIV_SYS_ADMIN},
    {"code": aerospike.PRIV_USER_ADMIN},
])

# Grant / revoke privileges
client.admin_grant_privileges("data_reader", [
    {"code": aerospike.PRIV_WRITE, "ns": "test", "set": "demo"},
])
client.admin_revoke_privileges("data_reader", [
    {"code": aerospike.PRIV_WRITE, "ns": "test", "set": "demo"},
])

# Whitelist and quotas
client.admin_set_whitelist("data_reader", ["10.0.0.0/8", "192.168.1.0/24"])
client.admin_set_quotas("data_reader", read_quota=1000, write_quota=500)

# Query / drop roles
role = client.admin_query_role("data_reader")
roles = client.admin_query_roles()
client.admin_drop_role("data_reader")
```

#### Privilege Codes

| Constant | Description |
|----------|-------------|
| `PRIV_READ` | Read records |
| `PRIV_WRITE` | Write records |
| `PRIV_READ_WRITE` | Read and write |
| `PRIV_READ_WRITE_UDF` | Read, write, and UDF |
| `PRIV_SYS_ADMIN` | System admin |
| `PRIV_USER_ADMIN` | User management |
| `PRIV_DATA_ADMIN` | Data management (truncate, index) |
| `PRIV_UDF_ADMIN` | UDF management |
| `PRIV_SINDEX_ADMIN` | Secondary index management |
| `PRIV_TRUNCATE` | Truncate operations |

#### Privilege Scope

```python
{"code": aerospike.PRIV_READ}                              # Global
{"code": aerospike.PRIV_READ, "ns": "test"}                # Namespace
{"code": aerospike.PRIV_READ, "ns": "test", "set": "demo"} # Namespace + set
```


---

### Error Handling

> Production error handling patterns for aerospike-py.

#### Catch Specific, Then Broad

```python
from aerospike_py.exception import (
    RecordNotFound,
    AerospikeTimeoutError,
    AerospikeError,
)

try:
    record = client.get(key)
except RecordNotFound:
    bins = {}
except AerospikeTimeoutError:
    raise  # retry or circuit-break
except AerospikeError as e:
    logger.error("Aerospike error: %s", e)
    raise
```

#### Retry with Backoff

```python
import time
from aerospike_py.exception import AerospikeTimeoutError, ClusterError

def get_with_retry(client, key, max_retries: int = 3):
    for attempt in range(max_retries):
        try:
            return client.get(key)
        except (AerospikeTimeoutError, ClusterError):
            if attempt == max_retries - 1:
                raise
            time.sleep(0.1 * (2 ** attempt))
```

#### Optimistic Locking (CAS)

```python
import aerospike_py as aerospike
from aerospike_py.exception import RecordGenerationError

def increment_counter(client, key, bin_name: str) -> int:
    while True:
        try:
            record = client.get(key)
            new_val = record.bins.get(bin_name, 0) + 1
            client.put(
                key,
                {bin_name: new_val},
                meta={"gen": record.meta.gen},
                policy={"gen": aerospike.POLICY_GEN_EQ},
            )
            return new_val
        except RecordGenerationError:
            continue
```

#### Create-Only vs Upsert

```python
from aerospike_py.exception import RecordExistsError

# Create-only
try:
    client.put(key, bins, policy={"exists": aerospike.POLICY_EXISTS_CREATE_ONLY})
except RecordExistsError:
    pass  # already exists

# Upsert (default) -- never raises RecordExistsError
client.put(key, bins)
```

#### Batch Error Handling

```python
batch = client.batch_read(keys)
for br in batch.batch_records:
    if br.result == aerospike.AEROSPIKE_OK and br.record:
        process(br.record.bins)
    elif br.result == aerospike.AEROSPIKE_ERR_RECORD_NOT_FOUND:
        handle_missing(br.key)
    else:
        logger.warning("Batch error: code=%d", br.result)
```

#### Connection Lifecycle

```python
from aerospike_py.exception import ClusterError

try:
    client = aerospike.client(config).connect()
except ClusterError as e:
    print(f"Cannot reach cluster: {e}")
    raise SystemExit(1)

try:
    # application logic
    pass
finally:
    client.close()
```

#### Async

Error handling is identical, just add `await`:

```python
async def get_user(client, user_id: str) -> dict | None:
    try:
        record = await client.get(("app", "users", user_id))
        return record.bins
    except RecordNotFound:
        return None
```

#### Result Code Reference

| Code | Constant | Exception |
|------|----------|-----------|
| 0 | `AEROSPIKE_OK` | (success) |
| 2 | `AEROSPIKE_ERR_RECORD_NOT_FOUND` | `RecordNotFound` |
| 5 | `AEROSPIKE_ERR_RECORD_EXISTS` | `RecordExistsError` |
| 9 | `AEROSPIKE_ERR_TIMEOUT` | `AerospikeTimeoutError` |
| 3 | `AEROSPIKE_ERR_RECORD_GENERATION` | `RecordGenerationError` |
| 13 | `AEROSPIKE_ERR_RECORD_TOO_BIG` | `RecordTooBig` |
| 27 | `AEROSPIKE_ERR_FILTERED_OUT` | `FilteredOut` |

See [Exceptions](/docs/api/exceptions) and [Constants](/docs/api/constants) for full lists.


---

### UDF Guide

> Register, execute, and remove Lua UDFs on the Aerospike server.

User Defined Functions (UDFs) are Lua scripts that execute on the Aerospike server node owning the record.

#### API

```python
# Register
client.udf_put("my_udf.lua")

# Execute on a record
result = client.apply(key, "module_name", "function_name", [arg1, arg2])

# Remove
client.udf_remove("module_name")
```

#### Example: Counter UDF

**`counter.lua`**

```lua
function increment(rec, bin_name, amount)
    if aerospike:exists(rec) then
        rec[bin_name] = rec[bin_name] + amount
        aerospike:update(rec)
    else
        rec[bin_name] = amount
        aerospike:create(rec)
    end
    return rec[bin_name]
end
```

**Python**

```python
client.udf_put("counter.lua")

key = ("test", "demo", "counter1")
result = client.apply(key, "counter", "increment", ["count", 5])  # 5
result = client.apply(key, "counter", "increment", ["count", 3])  # 8

client.udf_remove("counter")
```

**Async**

```python
await client.udf_put("counter.lua")
result = await client.apply(key, "counter", "increment", ["count", 1])
await client.udf_remove("counter")
```

#### Notes

- Lua is the only supported UDF language
- UDF changes take a few seconds to propagate to all nodes
- Keep UDFs simple for best performance


---

### Client Configuration

> Configure connections, timeouts, auth, and connection pools.

#### Basic Configuration

```python
import aerospike_py as aerospike
from aerospike_py.types import ClientConfig

config: ClientConfig = {
    "hosts": [("127.0.0.1", 3000)],
    "cluster_name": "docker",
}
client = aerospike.client(config).connect()
```

#### All Fields

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `hosts` | `list[tuple[str, int]]` | *required* | Seed node addresses |
| `cluster_name` | `str` | `""` | Expected cluster name |
| `auth_mode` | `int` | `AUTH_INTERNAL` | Auth mode |
| `user` / `password` | `str` | `""` | Credentials |
| `timeout` | `int` | `1000` | Connection timeout (ms) |
| `idle_timeout` | `int` | `55` | Idle connection timeout (s) |
| `max_conns_per_node` | `int` | `100` | Max connections per node |
| `min_conns_per_node` | `int` | `0` | Pre-warm connections |
| `tend_interval` | `int` | `1000` | Cluster tend interval (ms) |
| `use_services_alternate` | `bool` | `false` | Use alternate addresses |

#### Multi-Node Cluster

The client discovers all nodes from any reachable seed:

```python
config: ClientConfig = {
    "hosts": [
        ("node1.example.com", 3000),
        ("node2.example.com", 3000),
        ("node3.example.com", 3000),
    ],
}
```

#### Connection Pool

```python
config: ClientConfig = {
    "hosts": [("127.0.0.1", 3000)],
    "max_conns_per_node": 300,
    "min_conns_per_node": 10,
    "idle_timeout": 55,
}
```

- `max_conns_per_node`: Match to expected concurrent requests per node
- `min_conns_per_node`: Avoid cold-start latency
- `idle_timeout`: Keep below server `proto-fd-idle-ms` (default 60s)

#### Per-Operation Timeouts

```python
from aerospike_py.types import ReadPolicy, WritePolicy

read_policy: ReadPolicy = {
    "socket_timeout": 5000,
    "total_timeout": 10000,
    "max_retries": 2,
}
record = client.get(key, policy=read_policy)
```

#### Authentication

```python
# Internal
client = aerospike.client({
    "hosts": [("127.0.0.1", 3000)],
    "auth_mode": aerospike.AUTH_INTERNAL,
}).connect("admin", "admin")

# External (LDAP)
client = aerospike.client({
    "hosts": [("127.0.0.1", 3000)],
    "auth_mode": aerospike.AUTH_EXTERNAL,
}).connect("ldap_user", "ldap_pass")
```

#### Cluster Info

```python
from aerospike_py.types import InfoNodeResult

results: list[InfoNodeResult] = client.info_all("namespaces")
for r in results:
    print(f"{r.node_name}: {r.response}")

version: str = client.info_random_node("build")
```

#### Sync vs Async

##### Sync

```python
# Context manager (recommended)
with aerospike.client(config).connect() as client:
    record = client.get(key)
# close() called automatically

# Manual
client = aerospike.client(config).connect()
try:
    record = client.get(key)
finally:
    client.close()
```

##### Async

```python
# Context manager
async with aerospike.AsyncClient(config) as client:
    await client.connect()
    record = await client.get(key)

# Manual
client = aerospike.AsyncClient(config)
await client.connect()
try:
    record = await client.get(key)
finally:
    await client.close()
```

**Use async for:** High-concurrency web servers, fan-out reads, mixed I/O.
**Sync is fine for:** Scripts, batch jobs, sequential pipelines.


---

### Migration from Official Client

> Migrate from aerospike-client-python (C-based) to aerospike-py (Rust-based).

#### Installation

```bash
pip uninstall aerospike
pip install aerospike-py
```

#### Import Changes

```python
# Before
import aerospike
from aerospike import exception as ex

# After -- drop-in alias
import aerospike_py as aerospike
from aerospike_py import exception as ex
```

#### Client Creation

```python
# Identical API
config = {"hosts": [("127.0.0.1", 3000)]}
client = aerospike.client(config).connect()

# New: context manager
with aerospike.client(config).connect() as client:
    pass  # close() called automatically
```

#### CRUD -- Compatible

```python
key = ("test", "demo", "user1")

# Same signatures
client.put(key, {"name": "Alice", "age": 30})
_, meta, bins = client.get(key)
_, meta = client.exists(key)
client.remove(key)
client.select(key, ["name"])
client.touch(key)
client.append(key, "name", " Smith")
client.increment(key, "counter", 1)
```

#### Policies, Constants, Exceptions -- Compatible

```python
# Same policy dicts
policy = {"socket_timeout": 5000, "total_timeout": 10000, "max_retries": 2}

# Same constants
aerospike.POLICY_KEY_SEND       # 1
aerospike.TTL_NEVER_EXPIRE      # -1

# Same exception classes
from aerospike_py.exception import RecordNotFound, RecordExistsError
```

> **Exception Renames:**
> `TimeoutError` → `AerospikeTimeoutError`, `IndexError` → `AerospikeIndexError` to avoid shadowing Python builtins. Old names work as deprecated aliases.
#### CDT, Expressions, Query -- Compatible

```python
from aerospike_py import list_operations as lops, map_operations as mops, exp, predicates

# CDT operations
ops = [lops.list_append("tags", "new"), mops.map_put("attrs", "color", "blue")]
client.operate(key, ops)

# Expression filters
expr = exp.ge(exp.int_bin("age"), exp.int_val(18))
client.get(key, policy={"filter_expression": expr})

# Query
query = client.query("test", "demo")
query.where(predicates.between("age", 18, 65))
records = query.results()
```

#### Async Client (New)

Not available in the official client:

```python
import asyncio
import aerospike_py as aerospike

async def main():
    client = aerospike.AsyncClient({"hosts": [("127.0.0.1", 3000)]})
    await client.connect()
    await client.put(key, {"name": "Alice"})
    _, meta, bins = await client.get(key)
    await client.close()

asyncio.run(main())
```

#### Known Differences

| Feature | Official Client | aerospike-py |
|---------|----------------|--------------|
| Runtime | C extension | Rust + PyO3 |
| Async | No | Yes |
| NumPy batch reads | No | Yes |
| Context manager | No | Yes |
| `TimeoutError` | `TimeoutError` | `AerospikeTimeoutError` |
| `IndexError` | `IndexError` | `AerospikeIndexError` |
| `GeoJSON` type | `aerospike.GeoJSON` | Not yet available |


---

### Performance Tuning

> Tips for optimizing aerospike-py throughput and latency.

#### Connection Pool

```python
config = {
    "hosts": [("node1", 3000), ("node2", 3000)],
    "max_conns_per_node": 300,   # default: 100
    "min_conns_per_node": 10,    # pre-warm
    "idle_timeout": 55,          # below server proto-fd-idle-ms (60s)
}
```

#### Read Optimization

##### Select Specific Bins

```python
# Reads ALL bins from server
record = client.get(key)

# Reads only what you need (less network I/O)
record = client.select(key, ["name", "age"])
```

##### Use Batch Reads

```python
# N sequential round-trips
results = [client.get(k) for k in keys]

# Single round-trip
batch = client.batch_read(keys, bins=["name", "age"])
```

##### NumPy Batch Reads

For numeric workloads, skip Python dict overhead entirely:

```python
import numpy as np

dtype = np.dtype([("score", "i8"), ("rating", "f8")])
batch = client.batch_read(keys, bins=["score", "rating"], _dtype=dtype)
# batch.batch_records is a numpy structured array
```

See [NumPy Batch Guide](/docs/guides/crud/numpy-batch).

#### Write Optimization

##### Combine Operations

```python
# Two round-trips
client.put(key, {"counter": 1})
client.put(key, {"updated_at": now})

# Single round-trip
ops = [
    {"op": aerospike.OPERATOR_WRITE, "bin": "counter", "val": 1},
    {"op": aerospike.OPERATOR_WRITE, "bin": "updated_at", "val": now},
]
client.operate(key, ops)
```

##### TTL Strategy

```python
client.put(key, bins, meta={"ttl": aerospike.TTL_NEVER_EXPIRE})     # never expire
client.put(key, bins, meta={"ttl": aerospike.TTL_DONT_UPDATE})      # keep existing TTL
client.put(key, bins, meta={"ttl": aerospike.TTL_NAMESPACE_DEFAULT}) # use namespace default
```

#### Async Client

For high-concurrency workloads (web servers, fan-out reads):

```python
import asyncio

async def main() -> None:
    client = aerospike.AsyncClient({"hosts": [("127.0.0.1", 3000)]})
    await client.connect()

    keys = [("test", "demo", f"key{i}") for i in range(1000)]
    results = await asyncio.gather(*(client.get(k) for k in keys))

    await client.close()
```

#### Expression Filters

Push filtering to the server to reduce network transfer:

```python
from aerospike_py import exp

# Without filter: transfers ALL records, filters in Python
results = client.query("test", "demo").results()
active = [r for r in results if r.bins.get("active")]

# With filter: server returns only matching records
expr = exp.eq(exp.bool_bin("active"), exp.bool_val(True))
results = client.query("test", "demo").results(policy={"filter_expression": expr})
```

#### Timeout Guidelines

| Setting | Recommendation |
|---------|---------------|
| `socket_timeout` | 1-5s. Catches hung connections. |
| `total_timeout` | Set based on SLA. Includes retries. |
| `max_retries` | 2-3 for reads, 0 for writes (idempotency). |


---

### NumPy Batch Read Guide

> Use batch_read with numpy structured arrays for high-performance columnar analytics directly from Aerospike.

`batch_read()` with `_dtype` returns a **numpy structured array** instead of Python objects:

- **Zero-copy columnar access** -- `batch.batch_records["temperature"]` returns a numpy array
- **Vectorized computation** -- use numpy/pandas directly on results
- **Memory efficiency** -- Rust writes directly into numpy buffer, bypassing Python objects

> **Performance:**
> For 10K records with 5 bins, this eliminates ~60K intermediate Python objects compared to the standard `BatchRecords` path.
#### Installation

```bash
pip install "aerospike-py[numpy]"
```

This installs `numpy>=2.0` as an optional dependency.

#### Quick Start

##### Sync Client

```python
import numpy as np
import aerospike_py as aerospike

client = aerospike.client({
    "hosts": [("127.0.0.1", 3000)],
}).connect()

# 1. Write some records
for i in range(100):
    client.put(
        ("test", "sensors", f"sensor_{i}"),
        {"temperature": 20.0 + i * 0.5, "humidity": 40 + i, "status": 1},
        policy={"key": aerospike.POLICY_KEY_SEND},
    )

# 2. Define dtype matching your bins
dtype = np.dtype([
    ("temperature", "f8"),  # float64
    ("humidity", "i4"),     # int32
    ("status", "u1"),       # uint8
])

# 3. Batch read with _dtype
keys = [("test", "sensors", f"sensor_{i}") for i in range(100)]
batch = client.batch_read(keys, _dtype=dtype)

# 4. Access as numpy arrays
print(batch.batch_records["temperature"].mean())  # columnar access
print(batch.batch_records[0])                      # row access
print(batch.get("sensor_42")["temperature"])       # key lookup
```

##### Async Client

```python
import asyncio
import numpy as np
import aerospike_py as aerospike
from aerospike_py import AsyncClient

async def main():
    client = AsyncClient({
        "hosts": [("127.0.0.1", 3000)],
    })
    await client.connect()

    # 1. Write some records
    for i in range(100):
        await client.put(
            ("test", "sensors", f"sensor_{i}"),
            {"temperature": 20.0 + i * 0.5, "humidity": 40 + i, "status": 1},
            policy={"key": aerospike.POLICY_KEY_SEND},
        )

    # 2. Define dtype matching your bins
    dtype = np.dtype([
        ("temperature", "f8"),
        ("humidity", "i4"),
        ("status", "u1"),
    ])

    # 3. Batch read with _dtype
    keys = [("test", "sensors", f"sensor_{i}") for i in range(100)]
    batch = await client.batch_read(keys, _dtype=dtype)

    # 4. Access as numpy arrays
    print(batch.batch_records["temperature"].mean())
    print(batch.batch_records[0])
    print(batch.get("sensor_42")["temperature"])

    await client.close()

asyncio.run(main())
```

#### NumpyBatchRecords

When `_dtype` is provided, `batch_read()` returns a `NumpyBatchRecords` object:

| Attribute | Type | Description |
|-----------|------|-------------|
| `batch_records` | `np.ndarray` | Structured array with the user-specified dtype |
| `meta` | `np.ndarray` | Structured array with dtype `[("gen", "u4"), ("ttl", "u4")]` |
| `result_codes` | `np.ndarray` | `int32` array of per-record result codes (0 = success) |
| `_map` | `dict` | `{primary_key: index}` mapping for key-based lookup |

##### Methods

| Method | Returns | Description |
|--------|---------|-------------|
| `get(primary_key)` | `np.void` | Look up a single record by primary key |

#### Supported dtype Kinds

| numpy Kind | Code | Example | Aerospike Value |
|------------|------|---------|-----------------|
| Signed int | `i` | `"i1"`, `"i2"`, `"i4"`, `"i8"` | `Int(i64)` — truncated to target size |
| Unsigned int | `u` | `"u1"`, `"u2"`, `"u4"`, `"u8"` | `Int(i64)` — cast to unsigned |
| Float | `f` | `"f2"`, `"f4"`, `"f8"` | `Float(f64)` — cast to target precision |
| Fixed bytes | `S` | `"S8"`, `"S16"` | `Blob(bytes)` or `String` — truncated/zero-padded |
| Void bytes | `V` | `"V4"`, `"V16"` | `Blob(bytes)` — truncated/zero-padded |
| Sub-array | — | `("f4", (128,))` | `Blob(bytes)` — raw copy (e.g., vector embeddings) |

> **Unsupported dtypes:**
> Unicode strings (`U`) and Python objects (`O`) are rejected with `TypeError`. Use `S` (fixed bytes) for string data.
#### Access Patterns

##### Columnar Access

```python
temps = batch.batch_records["temperature"]  # float64 array
print(temps.mean(), temps.std(), temps.max())

# Boolean filtering
hot = batch.batch_records[temps > 40.0]
```

##### Row Access

```python
record = batch.batch_records[0]
print(record["temperature"], record["humidity"])
```

##### Key Lookup

```python
record = batch.get("sensor_42")
print(record["temperature"])
```

##### Meta Access

```python
# Generation and TTL per record
print(batch.meta["gen"])  # uint32 array
print(batch.meta["ttl"])  # uint32 array

# Check which records failed
failed = batch.result_codes != 0
print(f"Failed: {failed.sum()} / {len(batch.result_codes)}")
```

#### Defining dtype

The dtype field names must match your Aerospike bin names exactly.

##### Numeric Bins

```python
dtype = np.dtype([
    ("price", "f8"),       # float64
    ("quantity", "i4"),    # int32
    ("flags", "u1"),       # uint8
])
```

##### Bytes / Blob Bins

```python
dtype = np.dtype([
    ("name", "S32"),       # 32-byte fixed string
    ("raw_data", "V64"),   # 64-byte void buffer
])
```

##### Vector Embeddings (Sub-array)

Store float32 vectors (e.g., ML embeddings) as byte blobs in Aerospike, then read them back as sub-arrays:

##### Sync Client

```python
import numpy as np
import aerospike_py as aerospike

client = aerospike.client({"hosts": [("127.0.0.1", 3000)]}).connect()

dim = 128
dtype = np.dtype([
    ("embedding", "f4", (dim,)),  # 128-dim float32 sub-array
    ("score", "f4"),
])

# Write: store embedding as raw bytes
embedding = np.random.randn(dim).astype(np.float32)
client.put(
    ("test", "vectors", "vec_1"),
    {"embedding": embedding.tobytes(), "score": 0.95},
    policy={"key": aerospike.POLICY_KEY_SEND},
)

# Read: sub-array automatically reconstructed from bytes
keys = [("test", "vectors", "vec_1")]
batch = client.batch_read(keys, _dtype=dtype)

recovered = batch.batch_records[0]["embedding"]  # float32[128]
np.testing.assert_array_almost_equal(recovered, embedding)
```

##### Async Client

```python
import asyncio
import numpy as np
import aerospike_py as aerospike
from aerospike_py import AsyncClient

async def main():
    client = AsyncClient({"hosts": [("127.0.0.1", 3000)]})
    await client.connect()

    dim = 128
    dtype = np.dtype([
        ("embedding", "f4", (dim,)),
        ("score", "f4"),
    ])

    embedding = np.random.randn(dim).astype(np.float32)
    await client.put(
        ("test", "vectors", "vec_1"),
        {"embedding": embedding.tobytes(), "score": 0.95},
        policy={"key": aerospike.POLICY_KEY_SEND},
    )

    keys = [("test", "vectors", "vec_1")]
    batch = await client.batch_read(keys, _dtype=dtype)

    recovered = batch.batch_records[0]["embedding"]
    np.testing.assert_array_almost_equal(recovered, embedding)

    await client.close()

asyncio.run(main())
```

#### Bin Filtering

Combine `bins` and `_dtype` to read only specific bins from the server:

```python
dtype = np.dtype([("temperature", "f8")])
batch = client.batch_read(keys, bins=["temperature"], _dtype=dtype)
```

Only the `temperature` bin is transferred from the server, reducing network I/O.

#### Error Handling

##### Missing Records

Records not found (result code 2) are filled with zeros in the structured array:

```python
batch = client.batch_read(keys, _dtype=dtype)

# Check result codes
for i, rc in enumerate(batch.result_codes):
    if rc != 0:
        print(f"Record {i} failed with result code {rc}")

# Filter successful records only
success_mask = batch.result_codes == 0
valid_data = batch.batch_records[success_mask]
```

##### Missing Bins

If a record exists but a bin is missing, the field defaults to zero (the numpy zero-value for that dtype):

```python
# Record has "temperature" but not "humidity"
dtype = np.dtype([("temperature", "f8"), ("humidity", "i4")])
batch = client.batch_read(keys, _dtype=dtype)
# humidity will be 0 for records missing that bin
```

##### dtype Validation Errors

```python
# TypeError: unicode strings not supported
dtype = np.dtype([("name", "U10")])
batch = client.batch_read(keys, _dtype=dtype)  # raises TypeError

# TypeError: Python objects not supported
dtype = np.dtype([("data", "O")])
batch = client.batch_read(keys, _dtype=dtype)  # raises TypeError
```

#### Pandas Integration

Convert `NumpyBatchRecords` to a pandas DataFrame:

```python
import pandas as pd

batch = client.batch_read(keys, _dtype=dtype)

df = pd.DataFrame(batch.batch_records)
df["gen"] = batch.meta["gen"]
df["ttl"] = batch.meta["ttl"]

# Now use pandas operations
hot_sensors = df[df["temperature"] > 35.0]
print(hot_sensors.describe())
```

#### Best Practices

- **Match dtype to your bins** — field names in the dtype must match bin names in Aerospike
- **Use `bins` parameter** — combine with `_dtype` to reduce network transfer
- **Check `result_codes`** — filter out failed records before analysis
- **Use smallest sufficient dtype** — `"f4"` instead of `"f8"`, `"i2"` instead of `"i8"` to reduce memory
- **Batch size** — keep batches at 100-5,000 keys for optimal performance
- **Vector data** — store embeddings as `tobytes()` blobs, read with sub-array dtypes

#### API Reference

```python
# Sync
batch: NumpyBatchRecords = client.batch_read(
    keys: list[tuple[str, str, str | int | bytes]],
    bins: list[str] | None = None,
    policy: dict | None = None,
    _dtype: np.dtype = ...,
)

# Async
batch: NumpyBatchRecords = await client.batch_read(
    keys: list[tuple[str, str, str | int | bytes]],
    bins: list[str] | None = None,
    policy: dict | None = None,
    _dtype: np.dtype = ...,
)
```

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `keys` | `list[Key]` | required | List of `(namespace, set, primary_key)` tuples |
| `bins` | `list[str] \| None` | `None` | Bin names to read (`None` = all) |
| `policy` | `dict \| None` | `None` | Batch policy overrides |
| `_dtype` | `np.dtype` | required | Structured dtype defining output schema |


---

### NumPy Batch Write Guide

> Use batch_write_numpy to write records directly from numpy structured arrays for high-performance bulk ingestion into Aerospike.

#### Overview

`batch_write_numpy()` writes multiple records to Aerospike directly from a **numpy structured array**. Each row becomes a separate write operation, with dtype fields mapped to Aerospike bins.

- **Direct array-to-record mapping** — no intermediate Python dicts or loops
- **Key field extraction** — a designated dtype field (default `_key`) is used as the record's user key
- **Automatic bin mapping** — all non-underscore-prefixed fields become bins
- **Batch execution** — all rows are written in a single batch call

> **When to use:**
> Use `batch_write_numpy()` when your data is already in numpy arrays (e.g., ML feature stores, sensor data pipelines, scientific datasets). For regular Python dicts, use `put()` or standard batch operations instead.
#### Installation

```bash
pip install "aerospike-py[numpy]"
```

#### Quick Start

##### Sync Client

```python
import numpy as np
import aerospike_py as aerospike

client = aerospike.client({
    "hosts": [("127.0.0.1", 3000)],
}).connect()

# 1. Define dtype with key field and bin fields
dtype = np.dtype([
    ("_key", "i4"),     # record key (int32)
    ("score", "f8"),    # bin: float64
    ("count", "i4"),    # bin: int32
])

# 2. Create structured array
data = np.array([
    (1, 0.95, 10),
    (2, 0.87, 20),
    (3, 0.72, 15),
], dtype=dtype)

# 3. Batch write
results = client.batch_write_numpy(data, "test", "demo", dtype)

# 4. Check results
for record in results:
    key, meta, bins = record
    print(f"Key: {key}, Gen: {meta['gen']}")
```

##### Async Client

```python
import asyncio
import numpy as np
import aerospike_py as aerospike
from aerospike_py import AsyncClient

async def main():
    client = AsyncClient({
        "hosts": [("127.0.0.1", 3000)],
    })
    await client.connect()

    # 1. Define dtype with key field and bin fields
    dtype = np.dtype([
        ("_key", "i4"),
        ("score", "f8"),
        ("count", "i4"),
    ])

    # 2. Create structured array
    data = np.array([
        (1, 0.95, 10),
        (2, 0.87, 20),
        (3, 0.72, 15),
    ], dtype=dtype)

    # 3. Batch write
    results = await client.batch_write_numpy(data, "test", "demo", dtype)

    # 4. Check results
    for record in results:
        key, meta, bins = record
        print(f"Key: {key}, Gen: {meta['gen']}")

    await client.close()

asyncio.run(main())
```

#### How It Works

```
numpy structured array             Aerospike
┌──────┬───────┬───────┐
│ _key │ score │ count │
├──────┼───────┼───────┤          ┌──────────────────────┐
│  1   │ 0.95  │  10   │  ──────▶ │ key=1 {score, count} │
│  2   │ 0.87  │  20   │  ──────▶ │ key=2 {score, count} │
│  3   │ 0.72  │  15   │  ──────▶ │ key=3 {score, count} │
└──────┴───────┴───────┘          └──────────────────────┘
        ▲                                  ▲
   key_field="_key"               bins = non-underscore fields
```

1. The `key_field` (default `"_key"`) column is extracted as the user key for each record
2. All fields **not** prefixed with `_` become Aerospike bins
3. Fields prefixed with `_` (other than the key field) are ignored

#### Key Field

By default, the dtype field named `"_key"` is used as the record key. You can specify a different field with `key_field`:

##### Sync Client

```python
dtype = np.dtype([
    ("user_id", "i8"),    # use this as the record key
    ("score", "f8"),
])

data = np.array([(100, 1.5), (101, 2.5)], dtype=dtype)

# Use "user_id" as key instead of "_key"
results = client.batch_write_numpy(
    data, "test", "demo", dtype, key_field="user_id"
)
```

##### Async Client

```python
dtype = np.dtype([
    ("user_id", "i8"),
    ("score", "f8"),
])

data = np.array([(100, 1.5), (101, 2.5)], dtype=dtype)

results = await client.batch_write_numpy(
    data, "test", "demo", dtype, key_field="user_id"
)
```

> **Note:**
> When using a custom `key_field`, the field name should **not** start with `_` if you want it to also be stored as a bin. If the field starts with `_`, it is used only as the key and not written as a bin.
#### Supported dtype Kinds

The same dtype kinds supported by `batch_read()` with `_dtype` are supported for writes:

| numpy Kind | Code | Example | Aerospike Value |
|------------|------|---------|-----------------|
| Signed int | `i` | `"i1"`, `"i2"`, `"i4"`, `"i8"` | `Int(i64)` |
| Unsigned int | `u` | `"u1"`, `"u2"`, `"u4"`, `"u8"` | `Int(i64)` |
| Float | `f` | `"f4"`, `"f8"` | `Float(f64)` |
| Fixed bytes | `S` | `"S8"`, `"S16"` | `Blob(bytes)` or `String` |
| Void bytes | `V` | `"V4"`, `"V16"` | `Blob(bytes)` |
| Sub-array | — | `("f4", (128,))` | `Blob(bytes)` |

> **Unsupported dtypes:**
> Unicode strings (`U`) and Python objects (`O`) are not supported. Use `S` (fixed bytes) for string data.
#### Examples

##### Sensor Data Ingestion

##### Sync Client

```python
import numpy as np
import aerospike_py as aerospike

client = aerospike.client({"hosts": [("127.0.0.1", 3000)]}).connect()

dtype = np.dtype([
    ("_key", "i4"),
    ("temperature", "f8"),
    ("humidity", "f4"),
    ("pressure", "f4"),
    ("status", "u1"),
])

# Generate 1000 sensor readings
n = 1000
data = np.zeros(n, dtype=dtype)
data["_key"] = np.arange(n)
data["temperature"] = np.random.normal(25.0, 5.0, n)
data["humidity"] = np.random.uniform(30.0, 90.0, n).astype(np.float32)
data["pressure"] = np.random.normal(1013.25, 10.0, n).astype(np.float32)
data["status"] = 1

results = client.batch_write_numpy(data, "test", "sensors", dtype)
print(f"Wrote {len(results)} records")
```

##### Async Client

```python
import asyncio
import numpy as np
import aerospike_py as aerospike
from aerospike_py import AsyncClient

async def main():
    client = AsyncClient({"hosts": [("127.0.0.1", 3000)]})
    await client.connect()

    dtype = np.dtype([
        ("_key", "i4"),
        ("temperature", "f8"),
        ("humidity", "f4"),
        ("pressure", "f4"),
        ("status", "u1"),
    ])

    n = 1000
    data = np.zeros(n, dtype=dtype)
    data["_key"] = np.arange(n)
    data["temperature"] = np.random.normal(25.0, 5.0, n)
    data["humidity"] = np.random.uniform(30.0, 90.0, n).astype(np.float32)
    data["pressure"] = np.random.normal(1013.25, 10.0, n).astype(np.float32)
    data["status"] = 1

    results = await client.batch_write_numpy(data, "test", "sensors", dtype)
    print(f"Wrote {len(results)} records")

    await client.close()

asyncio.run(main())
```

##### Vector Embeddings

Store ML embeddings as byte blobs:

```python
import numpy as np
import aerospike_py as aerospike

client = aerospike.client({"hosts": [("127.0.0.1", 3000)]}).connect()

dim = 128
dtype = np.dtype([
    ("_key", "i4"),
    ("embedding", "V" + str(dim * 4)),  # 128 * 4 bytes = 512-byte blob
    ("label", "i4"),
])

n = 100
embeddings = np.random.randn(n, dim).astype(np.float32)

data = np.zeros(n, dtype=dtype)
data["_key"] = np.arange(n)
for i in range(n):
    data["embedding"][i] = embeddings[i].tobytes()
data["label"] = np.random.randint(0, 10, n)

results = client.batch_write_numpy(data, "test", "vectors", dtype)
```

##### Write and Read Roundtrip

Combine `batch_write_numpy()` and `batch_read()` with `_dtype` for a full numpy roundtrip:

##### Sync Client

```python
import numpy as np
import aerospike_py as aerospike

client = aerospike.client({"hosts": [("127.0.0.1", 3000)]}).connect()

# Define dtype
dtype = np.dtype([
    ("_key", "i4"),
    ("x", "f8"),
    ("y", "f8"),
    ("category", "i4"),
])

# Write
data = np.array([
    (1, 1.0, 2.0, 0),
    (2, 3.0, 4.0, 1),
    (3, 5.0, 6.0, 0),
], dtype=dtype)
client.batch_write_numpy(data, "test", "points", dtype)

# Read back with _dtype
read_dtype = np.dtype([("x", "f8"), ("y", "f8"), ("category", "i4")])
keys = [("test", "points", i) for i in range(1, 4)]
batch = client.batch_read(keys, _dtype=read_dtype, policy={"key": aerospike.POLICY_KEY_SEND})

# Vectorized analysis
print(batch.batch_records["x"].mean())       # 3.0
print(batch.batch_records["category"].sum())  # 1
```

##### Async Client

```python
import asyncio
import numpy as np
import aerospike_py as aerospike
from aerospike_py import AsyncClient

async def main():
    client = AsyncClient({"hosts": [("127.0.0.1", 3000)]})
    await client.connect()

    dtype = np.dtype([
        ("_key", "i4"),
        ("x", "f8"),
        ("y", "f8"),
        ("category", "i4"),
    ])

    data = np.array([
        (1, 1.0, 2.0, 0),
        (2, 3.0, 4.0, 1),
        (3, 5.0, 6.0, 0),
    ], dtype=dtype)
    await client.batch_write_numpy(data, "test", "points", dtype)

    read_dtype = np.dtype([("x", "f8"), ("y", "f8"), ("category", "i4")])
    keys = [("test", "points", i) for i in range(1, 4)]
    batch = await client.batch_read(keys, _dtype=read_dtype, policy={"key": aerospike.POLICY_KEY_SEND})

    print(batch.batch_records["x"].mean())
    print(batch.batch_records["category"].sum())

    await client.close()

asyncio.run(main())
```

##### Pandas DataFrame to Aerospike

Write a pandas DataFrame to Aerospike via numpy:

```python
import numpy as np
import pandas as pd
import aerospike_py as aerospike

client = aerospike.client({"hosts": [("127.0.0.1", 3000)]}).connect()

# DataFrame
df = pd.DataFrame({
    "user_id": [1, 2, 3],
    "score": [0.95, 0.87, 0.72],
    "level": [10, 20, 15],
})

# Convert to structured array
dtype = np.dtype([
    ("_key", "i4"),
    ("score", "f8"),
    ("level", "i4"),
])

data = np.zeros(len(df), dtype=dtype)
data["_key"] = df["user_id"].values
data["score"] = df["score"].values
data["level"] = df["level"].values

results = client.batch_write_numpy(data, "test", "users", dtype)
```

#### Error Handling

```python
from aerospike_py.exception import AerospikeError

try:
    results = client.batch_write_numpy(data, "test", "demo", dtype)
    for record in results:
        key, meta, bins = record
        if meta is None:
            print(f"Write failed for key {key}")
except AerospikeError as e:
    print(f"Batch write error: {e}")
```

#### Best Practices

- **Match dtype to your data** — use the smallest sufficient dtype (`"f4"` vs `"f8"`, `"i2"` vs `"i8"`) to reduce memory and network transfer
- **Batch size** — keep arrays at 100-5,000 rows per call for optimal performance
- **Key field convention** — use `"_key"` as the default key field to keep the convention consistent
- **Underscore prefix** — fields starting with `_` are excluded from bins, use this for metadata fields
- **Roundtrip with batch_read** — use the same dtype fields (minus `_key`) with `batch_read(_dtype=...)` for efficient read-back
- **Large datasets** — split large arrays into chunks and write in batches:

```python
chunk_size = 1000
for i in range(0, len(data), chunk_size):
    chunk = data[i:i + chunk_size]
    client.batch_write_numpy(chunk, "test", "demo", dtype)
```

#### API Reference

```python
# Sync
results: list[Record] = client.batch_write_numpy(
    data: np.ndarray,
    namespace: str,
    set_name: str,
    _dtype: np.dtype,
    key_field: str = "_key",
    policy: dict | None = None,
)

# Async
results: list[Record] = await client.batch_write_numpy(
    data: np.ndarray,
    namespace: str,
    set_name: str,
    _dtype: np.dtype,
    key_field: str = "_key",
    policy: dict | None = None,
)
```

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `data` | `np.ndarray` | required | Structured numpy array with record data |
| `namespace` | `str` | required | Target Aerospike namespace |
| `set_name` | `str` | required | Target set name |
| `_dtype` | `np.dtype` | required | Structured dtype describing the array layout |
| `key_field` | `str` | `"_key"` | Name of the dtype field to use as the record user key |
| `policy` | `dict \| None` | `None` | Optional [`BatchPolicy`](/docs/api/types#batchpolicy) overrides |

**Returns:** `list[Record]` — a list of `Record` NamedTuples `(key, meta, bins)` with write results.

**See also:** [NumPy Batch Read Guide](/docs/guides/crud/numpy-batch) for reading records back into numpy arrays.


---

### List & Map CDT Operations

> Atomic server-side List (31 ops) and Map (27 ops) collection data type operations via client.operate().

Atomic server-side collection data type (CDT) operations via `client.operate()`.

```python
from aerospike_py import list_operations as list_ops
from aerospike_py import map_operations as map_ops
import aerospike_py as aerospike
```

---

#### List CDT Operations

Each `list_ops.*` function returns an operation dict that you pass to `client.operate()` or `client.operate_ordered()`:

```python
ops = [
    list_ops.list_append("scores", 100),
    list_ops.list_size("scores"),
]
_, _, bins = client.operate(key, ops)
```

##### Basic Write Operations

##### list_append

**`list_append(bin, val, policy=None)`** — Append a value to the end of a list.

```python
ops = [list_ops.list_append("colors", "red")]
client.operate(key, ops)
```

##### list_append_items

**`list_append_items(bin, values, policy=None)`** — Append multiple values to a list.

```python
ops = [list_ops.list_append_items("colors", ["green", "blue"])]
client.operate(key, ops)
```

##### list_insert

**`list_insert(bin, index, val, policy=None)`** — Insert a value at the given index.

```python
ops = [list_ops.list_insert("colors", 0, "yellow")]
client.operate(key, ops)
```

**`list_insert_items(bin, index, values, policy=None)`** — Insert multiple values at the given index.

```python
ops = [list_ops.list_insert_items("colors", 1, ["cyan", "magenta"])]
client.operate(key, ops)
```

##### list_set

**`list_set(bin, index, val)`** — Set the value at a specific index.

```python
ops = [list_ops.list_set("colors", 0, "orange")]
client.operate(key, ops)
```

##### list_increment

**`list_increment(bin, index, val, policy=None)`** — Increment the numeric value at a given index.

```python
ops = [list_ops.list_increment("scores", 0, 10)]
client.operate(key, ops)
```

##### Basic Read Operations

###### `list_get(bin, index)`

Get the item at a specific index.

```python
ops = [list_ops.list_get("scores", 0)]
_, _, bins = client.operate(key, ops)
print(bins["scores"])  # first element
```

###### `list_get_range(bin, index, count)`

Get `count` items starting at `index`.

```python
ops = [list_ops.list_get_range("scores", 0, 3)]
_, _, bins = client.operate(key, ops)
print(bins["scores"])  # first 3 elements
```

###### `list_size(bin)`

Return the number of items in a list.

```python
ops = [list_ops.list_size("scores")]
_, _, bins = client.operate(key, ops)
print(bins["scores"])  # e.g., 5
```

##### Remove Operations

###### `list_remove(bin, index)`

Remove the item at the given index.

```python
ops = [list_ops.list_remove("colors", 0)]
client.operate(key, ops)
```

###### `list_remove_range(bin, index, count)`

Remove `count` items starting at `index`.

```python
ops = [list_ops.list_remove_range("colors", 1, 2)]
client.operate(key, ops)
```

###### `list_pop(bin, index)`

Remove and return the item at the given index.

```python
ops = [list_ops.list_pop("colors", 0)]
_, _, bins = client.operate(key, ops)
print(bins["colors"])  # the removed item
```

###### `list_pop_range(bin, index, count)`

Remove and return `count` items starting at `index`.

```python
ops = [list_ops.list_pop_range("colors", 0, 2)]
_, _, bins = client.operate(key, ops)
print(bins["colors"])  # list of removed items
```

###### `list_trim(bin, index, count)`

Remove items outside the specified range `[index, index+count)`.

```python
ops = [list_ops.list_trim("scores", 1, 3)]
client.operate(key, ops)
```

###### `list_clear(bin)`

Remove all items from a list.

```python
ops = [list_ops.list_clear("scores")]
client.operate(key, ops)
```

##### Sort & Order

###### `list_sort(bin, sort_flags=0)`

Sort the list in place.

```python
ops = [list_ops.list_sort("scores")]
client.operate(key, ops)

# Drop duplicates while sorting
ops = [list_ops.list_sort("scores", aerospike.LIST_SORT_DROP_DUPLICATES)]
client.operate(key, ops)
```

###### `list_set_order(bin, list_order=0)`

Set the list ordering type.

```python
ops = [list_ops.list_set_order("scores", aerospike.LIST_ORDERED)]
client.operate(key, ops)
```

##### Advanced Read Operations (by Value/Index/Rank)

These operations require a `return_type` parameter that controls what is returned.

###### `list_get_by_value(bin, val, return_type)`

Get items matching the given value.

```python
ops = [list_ops.list_get_by_value("tags", "urgent", aerospike.LIST_RETURN_INDEX)]
_, _, bins = client.operate(key, ops)
```

###### `list_get_by_value_list(bin, values, return_type)`

Get items matching any of the given values.

```python
ops = [list_ops.list_get_by_value_list(
    "tags", ["urgent", "important"], aerospike.LIST_RETURN_COUNT
)]
_, _, bins = client.operate(key, ops)
```

###### `list_get_by_value_range(bin, begin, end, return_type)`

Get items with values in the range `[begin, end)`.

```python
ops = [list_ops.list_get_by_value_range(
    "scores", 80, 100, aerospike.LIST_RETURN_VALUE
)]
_, _, bins = client.operate(key, ops)
```

###### `list_get_by_index(bin, index, return_type)`

Get item by index with specified return type.

```python
ops = [list_ops.list_get_by_index("scores", 0, aerospike.LIST_RETURN_VALUE)]
_, _, bins = client.operate(key, ops)
```

###### `list_get_by_index_range(bin, index, return_type, count=None)`

Get items by index range.

```python
ops = [list_ops.list_get_by_index_range(
    "scores", 2, aerospike.LIST_RETURN_VALUE, count=3
)]
_, _, bins = client.operate(key, ops)
```

###### `list_get_by_rank(bin, rank, return_type)`

Get item by rank (0 = smallest).

```python
ops = [list_ops.list_get_by_rank("scores", 0, aerospike.LIST_RETURN_VALUE)]
_, _, bins = client.operate(key, ops)
```

###### `list_get_by_rank_range(bin, rank, return_type, count=None)`

Get items by rank range.

```python
ops = [list_ops.list_get_by_rank_range(
    "scores", -3, aerospike.LIST_RETURN_VALUE, count=3
)]
_, _, bins = client.operate(key, ops)
```

##### Advanced Remove Operations (by Value/Index/Rank)

###### `list_remove_by_value(bin, val, return_type)`

Remove items matching the given value.

```python
ops = [list_ops.list_remove_by_value("tags", "temp", aerospike.LIST_RETURN_COUNT)]
_, _, bins = client.operate(key, ops)
```

###### `list_remove_by_value_list(bin, values, return_type)`

Remove items matching any of the given values.

```python
ops = [list_ops.list_remove_by_value_list(
    "tags", ["temp", "debug"], aerospike.LIST_RETURN_NONE
)]
client.operate(key, ops)
```

###### `list_remove_by_value_range(bin, begin, end, return_type)`

Remove items with values in the range `[begin, end)`.

```python
ops = [list_ops.list_remove_by_value_range(
    "scores", 0, 50, aerospike.LIST_RETURN_COUNT
)]
_, _, bins = client.operate(key, ops)
```

###### `list_remove_by_index(bin, index, return_type)`

Remove item by index.

```python
ops = [list_ops.list_remove_by_index("scores", 0, aerospike.LIST_RETURN_VALUE)]
_, _, bins = client.operate(key, ops)
```

###### `list_remove_by_index_range(bin, index, return_type, count=None)`

Remove items by index range.

```python
ops = [list_ops.list_remove_by_index_range(
    "scores", 0, aerospike.LIST_RETURN_NONE, count=2
)]
client.operate(key, ops)
```

###### `list_remove_by_rank(bin, rank, return_type)`

Remove item by rank.

```python
ops = [list_ops.list_remove_by_rank("scores", 0, aerospike.LIST_RETURN_VALUE)]
_, _, bins = client.operate(key, ops)
```

###### `list_remove_by_rank_range(bin, rank, return_type, count=None)`

Remove items by rank range.

```python
ops = [list_ops.list_remove_by_rank_range(
    "scores", 0, aerospike.LIST_RETURN_NONE, count=2
)]
client.operate(key, ops)
```

##### List Constants

| Constant | Description |
|----------|-------------|
| `LIST_RETURN_NONE` | Return nothing |
| `LIST_RETURN_INDEX` | Return index(es) |
| `LIST_RETURN_REVERSE_INDEX` | Return reverse index(es) |
| `LIST_RETURN_RANK` | Return rank(s) |
| `LIST_RETURN_REVERSE_RANK` | Return reverse rank(s) |
| `LIST_RETURN_COUNT` | Return count of matched items |
| `LIST_RETURN_VALUE` | Return value(s) |
| `LIST_RETURN_EXISTS` | Return boolean existence |
| `LIST_UNORDERED` | Unordered list (default) |
| `LIST_ORDERED` | Ordered list (maintains sort order) |
| `LIST_SORT_DEFAULT` | Default sort |
| `LIST_SORT_DROP_DUPLICATES` | Drop duplicates during sort |

##### List Complete Example

```python
import aerospike_py as aerospike
from aerospike_py import list_operations as list_ops

with aerospike.client({
    "hosts": [("127.0.0.1", 3000)],
    "cluster_name": "docker",
}).connect() as client:

    key = ("test", "demo", "player1")

    # Initialize a scores list
    client.put(key, {"scores": [85, 92, 78, 95, 88]})

    # Atomic: sort, get top 3, and get size
    ops = [
        list_ops.list_sort("scores"),
        list_ops.list_get_by_rank_range(
            "scores", -3, aerospike.LIST_RETURN_VALUE, count=3
        ),
    ]
    _, _, bins = client.operate(key, ops)
    print(f"Top 3 scores: {bins['scores']}")

    # Remove scores below 80
    ops = [
        list_ops.list_remove_by_value_range(
            "scores", 0, 80, aerospike.LIST_RETURN_COUNT
        ),
    ]
    _, _, bins = client.operate(key, ops)
    print(f"Removed {bins['scores']} low scores")

    # Append a new score and get updated size
    ops = [
        list_ops.list_append("scores", 97),
        list_ops.list_size("scores"),
    ]
    _, _, bins = client.operate(key, ops)
    print(f"Total scores: {bins['scores']}")
```

---

#### Map CDT Operations

Each `map_ops.*` function returns an operation dict that you pass to `client.operate()` or `client.operate_ordered()`:

```python
ops = [
    map_ops.map_put("profile", "email", "alice@example.com"),
    map_ops.map_size("profile"),
]
_, _, bins = client.operate(key, ops)
```

##### Basic Write Operations

##### map_put

**`map_put(bin, key, val, policy=None)`** — Put a key/value pair into a map.

```python
ops = [map_ops.map_put("profile", "name", "Alice")]
client.operate(key, ops)
```

##### map_put_items

**`map_put_items(bin, items, policy=None)`** — Put multiple key/value pairs into a map.

```python
ops = [map_ops.map_put_items("profile", {
    "name": "Alice",
    "email": "alice@example.com",
    "age": 30,
})]
client.operate(key, ops)
```

##### map_increment

**`map_increment(bin, key, incr, policy=None)`** — Increment a numeric value in a map by key.

```python
ops = [map_ops.map_increment("counters", "views", 1)]
client.operate(key, ops)
```

##### map_decrement

**`map_decrement(bin, key, decr, policy=None)`** — Decrement a numeric value in a map by key.

```python
ops = [map_ops.map_decrement("counters", "stock", 1)]
client.operate(key, ops)
```

##### Basic Read Operations

###### `map_size(bin)`

Return the number of entries in a map.

```python
ops = [map_ops.map_size("profile")]
_, _, bins = client.operate(key, ops)
print(bins["profile"])  # e.g., 3
```

###### `map_get_by_key(bin, key, return_type)`

Get an entry by key.

```python
ops = [map_ops.map_get_by_key("profile", "name", aerospike.MAP_RETURN_VALUE)]
_, _, bins = client.operate(key, ops)
print(bins["profile"])  # "Alice"
```

##### Map Settings

###### `map_set_order(bin, map_order)`

Set the map ordering type.

```python
ops = [map_ops.map_set_order("profile", aerospike.MAP_KEY_ORDERED)]
client.operate(key, ops)
```

###### `map_clear(bin)`

Remove all items from a map.

```python
ops = [map_ops.map_clear("profile")]
client.operate(key, ops)
```

##### Remove Operations

###### `map_remove_by_key(bin, key, return_type)`

Remove entry by key.

```python
ops = [map_ops.map_remove_by_key("profile", "temp", aerospike.MAP_RETURN_NONE)]
client.operate(key, ops)
```

###### `map_remove_by_key_list(bin, keys, return_type)`

Remove entries matching any of the given keys.

```python
ops = [map_ops.map_remove_by_key_list(
    "profile", ["temp", "debug"], aerospike.MAP_RETURN_COUNT
)]
_, _, bins = client.operate(key, ops)
```

###### `map_remove_by_key_range(bin, begin, end, return_type)`

Remove entries with keys in the range `[begin, end)`.

```python
ops = [map_ops.map_remove_by_key_range(
    "cache", "tmp_a", "tmp_z", aerospike.MAP_RETURN_NONE
)]
client.operate(key, ops)
```

###### `map_remove_by_value(bin, val, return_type)`

Remove entries by value.

```python
ops = [map_ops.map_remove_by_value("scores", 0, aerospike.MAP_RETURN_KEY)]
_, _, bins = client.operate(key, ops)
```

###### `map_remove_by_value_list(bin, values, return_type)`

Remove entries matching any of the given values.

```python
ops = [map_ops.map_remove_by_value_list(
    "tags", ["deprecated", "old"], aerospike.MAP_RETURN_NONE
)]
client.operate(key, ops)
```

###### `map_remove_by_value_range(bin, begin, end, return_type)`

Remove entries with values in the range `[begin, end)`.

```python
ops = [map_ops.map_remove_by_value_range(
    "scores", 0, 50, aerospike.MAP_RETURN_COUNT
)]
_, _, bins = client.operate(key, ops)
```

##### Advanced Read Operations (by Key/Value/Index/Rank)

These operations require a `return_type` parameter that controls what is returned.

###### `map_get_by_key_range(bin, begin, end, return_type)`

Get entries with keys in the range `[begin, end)`.

```python
ops = [map_ops.map_get_by_key_range(
    "profile", "a", "n", aerospike.MAP_RETURN_KEY_VALUE
)]
_, _, bins = client.operate(key, ops)
```

###### `map_get_by_key_list(bin, keys, return_type)`

Get entries matching any of the given keys.

```python
ops = [map_ops.map_get_by_key_list(
    "profile", ["name", "email"], aerospike.MAP_RETURN_VALUE
)]
_, _, bins = client.operate(key, ops)
```

###### `map_get_by_value(bin, val, return_type)`

Get entries by value.

```python
ops = [map_ops.map_get_by_value("scores", 100, aerospike.MAP_RETURN_KEY)]
_, _, bins = client.operate(key, ops)
```

###### `map_get_by_value_range(bin, begin, end, return_type)`

Get entries with values in the range `[begin, end)`.

```python
ops = [map_ops.map_get_by_value_range(
    "scores", 90, 100, aerospike.MAP_RETURN_KEY_VALUE
)]
_, _, bins = client.operate(key, ops)
```

###### `map_get_by_value_list(bin, values, return_type)`

Get entries matching any of the given values.

```python
ops = [map_ops.map_get_by_value_list(
    "scores", [100, 95], aerospike.MAP_RETURN_KEY
)]
_, _, bins = client.operate(key, ops)
```

###### `map_get_by_index(bin, index, return_type)`

Get entry by index (key-ordered position).

```python
ops = [map_ops.map_get_by_index("profile", 0, aerospike.MAP_RETURN_KEY_VALUE)]
_, _, bins = client.operate(key, ops)
```

###### `map_get_by_index_range(bin, index, return_type, count=None)`

Get entries by index range.

```python
ops = [map_ops.map_get_by_index_range(
    "profile", 0, aerospike.MAP_RETURN_KEY_VALUE, count=3
)]
_, _, bins = client.operate(key, ops)
```

###### `map_get_by_rank(bin, rank, return_type)`

Get entry by rank (0 = smallest value).

```python
ops = [map_ops.map_get_by_rank("scores", 0, aerospike.MAP_RETURN_KEY_VALUE)]
_, _, bins = client.operate(key, ops)
```

###### `map_get_by_rank_range(bin, rank, return_type, count=None)`

Get entries by rank range.

```python
ops = [map_ops.map_get_by_rank_range(
    "scores", -3, aerospike.MAP_RETURN_KEY_VALUE, count=3
)]
_, _, bins = client.operate(key, ops)
```

##### Advanced Remove Operations (by Index/Rank)

###### `map_remove_by_index(bin, index, return_type)`

Remove entry by index.

```python
ops = [map_ops.map_remove_by_index("profile", 0, aerospike.MAP_RETURN_KEY_VALUE)]
_, _, bins = client.operate(key, ops)
```

###### `map_remove_by_index_range(bin, index, return_type, count=None)`

Remove entries by index range.

```python
ops = [map_ops.map_remove_by_index_range(
    "cache", 0, aerospike.MAP_RETURN_NONE, count=5
)]
client.operate(key, ops)
```

###### `map_remove_by_rank(bin, rank, return_type)`

Remove entry by rank.

```python
ops = [map_ops.map_remove_by_rank("scores", 0, aerospike.MAP_RETURN_VALUE)]
_, _, bins = client.operate(key, ops)
```

###### `map_remove_by_rank_range(bin, rank, return_type, count=None)`

Remove entries by rank range.

```python
ops = [map_ops.map_remove_by_rank_range(
    "scores", 0, aerospike.MAP_RETURN_NONE, count=2
)]
client.operate(key, ops)
```

##### Map Constants

| Constant | Description |
|----------|-------------|
| `MAP_RETURN_NONE` | Return nothing |
| `MAP_RETURN_INDEX` | Return index(es) |
| `MAP_RETURN_REVERSE_INDEX` | Return reverse index(es) |
| `MAP_RETURN_RANK` | Return rank(s) |
| `MAP_RETURN_REVERSE_RANK` | Return reverse rank(s) |
| `MAP_RETURN_COUNT` | Return count of matched entries |
| `MAP_RETURN_KEY` | Return key(s) |
| `MAP_RETURN_VALUE` | Return value(s) |
| `MAP_RETURN_KEY_VALUE` | Return key-value pair(s) |
| `MAP_RETURN_EXISTS` | Return boolean existence |
| `MAP_UNORDERED` | Unordered map (default) |
| `MAP_KEY_ORDERED` | Ordered by key |
| `MAP_KEY_VALUE_ORDERED` | Ordered by key and value |
| `MAP_WRITE_FLAGS_DEFAULT` | Default behavior |
| `MAP_WRITE_FLAGS_CREATE_ONLY` | Only create new entries |
| `MAP_WRITE_FLAGS_UPDATE_ONLY` | Only update existing entries |
| `MAP_WRITE_FLAGS_NO_FAIL` | Do not raise error on policy violation |
| `MAP_WRITE_FLAGS_PARTIAL` | Allow partial success for multi-item ops |

##### Map Complete Example

```python
import aerospike_py as aerospike
from aerospike_py import map_operations as map_ops

with aerospike.client({
    "hosts": [("127.0.0.1", 3000)],
    "cluster_name": "docker",
}).connect() as client:

    key = ("test", "demo", "player1")

    # Initialize a scores map
    client.put(key, {"scores": {"math": 92, "science": 88, "english": 75, "art": 95}})

    # Atomic: get top 2 scores and total count
    ops = [
        map_ops.map_get_by_rank_range(
            "scores", -2, aerospike.MAP_RETURN_KEY_VALUE, count=2
        ),
    ]
    _, _, bins = client.operate(key, ops)
    print(f"Top 2 scores: {bins['scores']}")

    # Remove scores below 80
    ops = [
        map_ops.map_remove_by_value_range(
            "scores", 0, 80, aerospike.MAP_RETURN_KEY
        ),
    ]
    _, _, bins = client.operate(key, ops)
    print(f"Removed subjects: {bins['scores']}")

    # Add a new score and increment an existing one
    ops = [
        map_ops.map_put("scores", "history", 90),
        map_ops.map_increment("scores", "math", 5),
        map_ops.map_size("scores"),
    ]
    _, _, bins = client.operate(key, ops)
    print(f"Total subjects: {bins['scores']}")
```


---

### Read Operations

> Get, select, exists, and batch read operations.

#### Keys

Every record is identified by a key tuple: `(namespace, set, primary_key)`.

```python
key = ("test", "demo", "user1")      # string PK
key = ("test", "demo", 12345)         # integer PK
key = ("test", "demo", b"\x01\x02")   # bytes PK
```

#### Read

##### Sync

```python
from aerospike_py import Record

record: Record = client.get(key)
print(record.bins)       # {"name": "Alice", "age": 30}
print(record.meta.gen)   # 1
print(record.meta.ttl)   # 2591998

# Tuple unpacking (backward compat)
_, meta, bins = client.get(key)

# Read specific bins
record = client.select(key, ["name"])
# record.bins = {"name": "Alice"}
```

##### Async

```python
record: Record = await client.get(key)
_, meta, bins = await client.get(key)
record = await client.select(key, ["name"])
```

#### Exists

```python
from aerospike_py import ExistsResult

result: ExistsResult = client.exists(key)  # or: await client.exists(key)
if result.meta is not None:
    print(f"gen={result.meta.gen}")
```

#### Batch Read

Read multiple records in a single network call.

##### Sync

```python
keys: list[tuple] = [("test", "demo", f"user_{i}") for i in range(10)]

# All bins
batch = client.batch_read(keys)
for br in batch.batch_records:
    if br.record:
        print(br.record.bins)

# Specific bins
batch = client.batch_read(keys, bins=["name", "age"])

# Existence check only
batch = client.batch_read(keys, bins=[])
```

##### Async

```python
batch = await client.batch_read(keys, bins=["name", "age"])
for br in batch.batch_records:
    if br.record:
        print(br.record.bins)
```

#### Tips

- **Batch size**: 100-5,000 keys per batch is optimal. Very large batches may timeout.
- **Timeouts**: Increase `total_timeout` for large batch operations.
- **Error handling**: Individual batch records can fail independently. Always check `br.record` for `None`.


---

### Write Operations

> Put, update, delete, operate, batch operate, and optimistic locking.

#### Write

##### Sync

```python
import aerospike_py as aerospike

client = aerospike.client({"hosts": [("127.0.0.1", 3000)]}).connect()
key: tuple[str, str, str] = ("test", "demo", "user1")

# Simple write
client.put(key, {"name": "Alice", "age": 30})

# Supported types: str, int, float, bytes, list, dict, bool, None
client.put(key, {
    "str_bin": "hello",
    "int_bin": 42,
    "float_bin": 3.14,
    "list_bin": [1, 2, 3],
    "map_bin": {"nested": "dict"},
})

# With TTL
client.put(key, {"val": 1}, meta={"ttl": 300})

# Create only (fail if exists)
client.put(key, {"val": 1}, policy={"exists": aerospike.POLICY_EXISTS_CREATE_ONLY})
```

##### Async

```python
await client.put(key, {"name": "Alice", "age": 30})
await client.put(key, {"val": 1}, meta={"ttl": 300})
await client.put(key, {"val": 1}, policy={"exists": aerospike.POLICY_EXISTS_CREATE_ONLY})
```

#### Update

##### Sync

```python
client.increment(key, "age", 1)
client.increment(key, "score", 0.5)
client.append(key, "name", " Smith")
client.prepend(key, "greeting", "Hello, ")
```

##### Async

```python
await client.increment(key, "age", 1)
await client.append(key, "name", " Smith")
```

#### Delete

##### Sync

```python
client.remove(key)

# With generation check
client.remove(key, meta={"gen": 5}, policy={"gen": aerospike.POLICY_GEN_EQ})

# Remove specific bins
client.remove_bin(key, ["temp_bin", "debug_bin"])
```

##### Async

```python
await client.remove(key)
await client.remove_bin(key, ["temp_bin"])
```

#### Touch (Reset TTL)

```python
client.touch(key, val=600)  # or: await client.touch(key, val=600)
```

#### Multi-Operation (Operate)

Execute multiple operations atomically on a single record.

##### Sync

```python
ops: list[dict] = [
    {"op": aerospike.OPERATOR_WRITE, "bin": "name", "val": "Bob"},
    {"op": aerospike.OPERATOR_INCR, "bin": "counter", "val": 1},
    {"op": aerospike.OPERATOR_READ, "bin": "counter", "val": None},
]
record = client.operate(key, ops)
print(record.bins["counter"])

# Ordered results
result = client.operate_ordered(key, ops)
for bt in result.ordered_bins:
    print(f"{bt.name} = {bt.value}")
```

##### Async

```python
record = await client.operate(key, ops)
result = await client.operate_ordered(key, ops)
```

#### Batch Operate / Remove

```python
# Batch operate
ops = [{"op": aerospike.OPERATOR_INCR, "bin": "views", "val": 1}]
results: list[Record] = client.batch_operate(keys, ops)

# Batch remove
results = client.batch_remove(keys)
```

#### Optimistic Locking

```python
from aerospike_py.exception import RecordGenerationError

record = client.get(key)
try:
    client.put(
        key,
        {"val": record.bins["val"] + 1},
        meta={"gen": record.meta.gen},
        policy={"gen": aerospike.POLICY_GEN_EQ},
    )
except RecordGenerationError:
    print("Concurrent modification, retry needed")
```

#### Tips

- **Batch size**: 100-5,000 keys per batch is optimal. Very large batches may timeout.
- **Timeouts**: Increase `total_timeout` for large batch operations.
- **Error handling**: Individual batch records can fail independently. Always check `br.record` for `None`.


---

### Expression Filters

> Server-side record filtering with 104+ composable expression functions.

Server-side filtering during read, write, and query operations. The server evaluates the expression and only returns (or modifies) matching records.

> **Server Requirement:**
> Expression filters require Aerospike Server **5.2+**.
#### Import

```python
from aerospike_py import exp
```

#### Basic Usage

```python
# Build expression: age >= 21
expr = exp.ge(exp.int_bin("age"), exp.int_val(21))

# Use in any operation via policy
record = client.get(key, policy={"filter_expression": expr})
```

#### Value Constructors

| Function | Description |
|----------|-------------|
| `int_val(v)` | 64-bit integer |
| `float_val(v)` | 64-bit float |
| `string_val(v)` | String |
| `bool_val(v)` | Boolean |
| `blob_val(v)` | Bytes |
| `list_val(v)` | List |
| `map_val(v)` | Map/dict |
| `geo_val(v)` | GeoJSON string |
| `nil()` | Nil |
| `infinity()` | Infinity (unbounded ranges) |
| `wildcard()` | Wildcard |

#### Bin Accessors

| Function | Description |
|----------|-------------|
| `int_bin(name)` | Integer bin |
| `float_bin(name)` | Float bin |
| `string_bin(name)` | String bin |
| `bool_bin(name)` | Boolean bin |
| `blob_bin(name)` | Blob bin |
| `list_bin(name)` | List bin |
| `map_bin(name)` | Map bin |
| `geo_bin(name)` | Geospatial bin |
| `hll_bin(name)` | HyperLogLog bin |
| `bin_exists(name)` | True if bin exists |
| `bin_type(name)` | Bin particle type |

#### Comparison

| Function | Operator |
|----------|----------|
| `eq(l, r)` | `==` |
| `ne(l, r)` | `!=` |
| `gt(l, r)` | `>` |
| `ge(l, r)` | `>=` |
| `lt(l, r)` | `<` |
| `le(l, r)` | `<=` |

#### Logic

| Function | Description |
|----------|-------------|
| `and_(*exprs)` | Logical AND |
| `or_(*exprs)` | Logical OR |
| `not_(expr)` | Logical NOT |
| `xor_(*exprs)` | Logical XOR |

```python
# age >= 18 AND active == true
exp.and_(
    exp.ge(exp.int_bin("age"), exp.int_val(18)),
    exp.eq(exp.bool_bin("active"), exp.bool_val(True)),
)

# NOT deleted
exp.not_(exp.eq(exp.bool_bin("deleted"), exp.bool_val(True)))
```

#### Numeric Operations

| Function | Description |
|----------|-------------|
| `num_add`, `num_sub`, `num_mul`, `num_div` | Arithmetic |
| `num_mod`, `num_pow`, `num_log` | Modulo, power, log |
| `num_abs`, `num_floor`, `num_ceil` | Absolute, floor, ceil |
| `to_int`, `to_float` | Type conversion |
| `min_`, `max_` | Min/max |

```python
# (price * quantity) > 1000
exp.gt(
    exp.num_mul(exp.int_bin("price"), exp.int_bin("quantity")),
    exp.int_val(1000),
)
```

#### Record Metadata

| Function | Description |
|----------|-------------|
| `key(exp_type)` | Primary key |
| `key_exists()` | Key stored in metadata? |
| `set_name()` | Set name |
| `record_size()` | Size in bytes (Server 7.0+) |
| `last_update()` | Last update (ns since epoch) |
| `since_update()` | Ms since last update |
| `void_time()` | Expiration (ns since epoch) |
| `ttl()` | TTL in seconds |
| `is_tombstone()` | Tombstone record? |
| `digest_modulo(mod)` | Digest modulo (sampling) |

```python
# Expiring within 1 hour
exp.lt(exp.ttl(), exp.int_val(3600))

# Sample ~10% of records
exp.eq(exp.digest_modulo(10), exp.int_val(0))
```

#### Pattern Matching

```python
# Regex (flags=2 for case insensitive)
exp.regex_compare("^alice.*", 2, exp.string_bin("name"))

# Geospatial: point within circle
region = '{"type":"AeroCircle","coordinates":[[-122.0, 37.5], 1000]}'
exp.geo_compare(exp.geo_bin("location"), exp.geo_val(region))
```

#### Variables and Control Flow

```python
# Conditional
exp.cond(
    exp.lt(exp.int_bin("age"), exp.int_val(18)), exp.string_val("minor"),
    exp.lt(exp.int_bin("age"), exp.int_val(65)), exp.string_val("adult"),
    exp.string_val("senior"),
)

# Let bindings
exp.let_(
    exp.def_("total", exp.num_mul(exp.int_bin("price"), exp.int_bin("qty"))),
    exp.gt(exp.var("total"), exp.int_val(1000)),
)
```

#### Using with Operations

##### Get / Put

```python
expr = exp.ge(exp.int_bin("age"), exp.int_val(21))

# Get: raises FilteredOut if no match
record = client.get(key, policy={"filter_expression": expr})

# Put: only update if status == "active"
expr = exp.eq(exp.string_bin("status"), exp.string_val("active"))
client.put(key, {"visits": 1}, policy={"filter_expression": expr})
```

##### Query

```python
query = client.query("test", "demo")
query.where(predicates.between("age", 20, 50))

expr = exp.eq(exp.string_bin("region"), exp.string_val("US"))
records = query.results(policy={"filter_expression": expr})
```

##### Batch

```python
expr = exp.ge(exp.int_bin("score"), exp.int_val(100))
ops = [{"op": aerospike.OPERATOR_READ, "bin": "score", "val": None}]
records = client.batch_operate(keys, ops, policy={"filter_expression": expr})
```

#### Practical Examples

```python
# Active premium users
expr = exp.and_(
    exp.eq(exp.bool_bin("active"), exp.bool_val(True)),
    exp.or_(
        exp.eq(exp.string_bin("tier"), exp.string_val("gold")),
        exp.eq(exp.string_bin("tier"), exp.string_val("platinum")),
    ),
    exp.ge(exp.int_bin("age"), exp.int_val(18)),
)
records = client.query("test", "users").results(policy={"filter_expression": expr})

# Records expiring within 1 hour
expr = exp.and_(
    exp.gt(exp.ttl(), exp.int_val(0)),
    exp.lt(exp.ttl(), exp.int_val(3600)),
)
expiring = client.query("test", "cache").results(policy={"filter_expression": expr})

# High-value transactions
expr = exp.gt(
    exp.num_mul(exp.float_bin("amount"), exp.int_bin("quantity")),
    exp.float_val(10000.0),
)
records = client.query("test", "transactions").results(policy={"filter_expression": expr})
```


---

### Query Guide

> Secondary index queries with predicates.

#### Secondary Index Queries

Queries require a secondary index on the bin being queried.

##### Create Index and Insert Data

```python
import aerospike_py as aerospike

client = aerospike.client({"hosts": [("127.0.0.1", 3000)]}).connect()

# Create indexes
client.index_integer_create("test", "users", "age", "users_age_idx")
client.index_string_create("test", "users", "city", "users_city_idx")

# Insert data
for i in range(100):
    client.put(("test", "users", f"user_{i}"), {
        "name": f"User {i}",
        "age": 20 + (i % 40),
        "city": ["Seoul", "Tokyo", "NYC"][i % 3],
    })
```

##### Query with Predicates

```python
from aerospike_py import predicates, Record

# Equality
query = client.query("test", "users")
query.where(predicates.equals("city", "Seoul"))
records: list[Record] = query.results()

# Range
query = client.query("test", "users")
query.select("name", "age")
query.where(predicates.between("age", 25, 35))
records = query.results()
```

##### Callback Iteration

```python
def process(record: Record) -> None:
    print(f"{record.bins['name']}: age {record.bins['age']}")

query = client.query("test", "users")
query.where(predicates.between("age", 25, 35))
query.foreach(process)
```

Return `False` from the callback to stop early:

```python
count = 0

def limited(record: Record):
    global count
    count += 1
    if count >= 5:
        return False  # stop iteration

query.foreach(limited)
```

##### Cleanup

```python
client.index_remove("test", "users_age_idx")
client.index_remove("test", "users_city_idx")
```

#### Predicate Reference

| Function | Description |
|----------|-------------|
| `equals(bin, val)` | Equality match |
| `between(bin, min, max)` | Range (inclusive) |
| `contains(bin, idx_type, val)` | List/map contains |
| `geo_within_geojson_region(bin, geojson)` | Points in region |
| `geo_within_radius(bin, lat, lng, radius)` | Points in circle (meters) |
| `geo_contains_geojson_point(bin, geojson)` | Regions containing point |

##### Geospatial

```python
# Points within a polygon
region = '{"type":"Polygon","coordinates":[[[126.9,37.5],[126.9,37.6],[127.0,37.6],[127.0,37.5],[126.9,37.5]]]}'
query.where(predicates.geo_within_geojson_region("location", region))

# Points within radius (meters)
query.where(predicates.geo_within_radius("location", 37.5665, 126.978, 5000.0))

# Regions containing a point
point = '{"type":"Point","coordinates":[126.978, 37.5665]}'
query.where(predicates.geo_contains_geojson_point("coverage", point))
```

See [Expression Filters](/docs/guides/query-scan/expression-filters) for server-side filtering without secondary indexes.


---

## Integrations

### FastAPI Integration

> AsyncClient with FastAPI lifespan and dependency injection.

#### Prerequisites

```bash
pip install fastapi uvicorn pydantic-settings aerospike-py
```

#### Lifespan Management

```python
from contextlib import asynccontextmanager

import aerospike_py
from aerospike_py import AsyncClient
from fastapi import FastAPI

@asynccontextmanager
async def lifespan(app: FastAPI):
    client = AsyncClient({
        "hosts": [("127.0.0.1", 3000)],
        "policies": {"key": aerospike_py.POLICY_KEY_SEND},
    })
    await client.connect()
    app.state.aerospike = client
    yield
    await client.close()

app = FastAPI(lifespan=lifespan)
```

#### Dependency Injection

```python
from aerospike_py import AsyncClient
from fastapi import Request

def get_client(request: Request) -> AsyncClient:
    return request.app.state.aerospike
```

#### Configuration

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    aerospike_host: str = "127.0.0.1"
    aerospike_port: int = 3000
    aerospike_namespace: str = "test"
    aerospike_set: str = "users"

    model_config = {"env_prefix": "APP_"}
```

#### CRUD Endpoint Example

```python
import uuid

from aerospike_py import AsyncClient
from aerospike_py.exception import RecordNotFound
from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel, Field

NS, SET = "test", "users"
router = APIRouter(prefix="/users", tags=["users"])

class UserCreate(BaseModel):
    name: str = Field(..., min_length=1, max_length=128)
    email: str
    age: int = Field(..., ge=0, le=200)

class UserResponse(BaseModel):
    user_id: str
    name: str
    email: str
    age: int
    generation: int

def _client(request: Request) -> AsyncClient:
    return request.app.state.aerospike

def _key(user_id: str) -> tuple[str, str, str]:
    return (NS, SET, user_id)

@router.post("", response_model=UserResponse, status_code=201)
async def create_user(body: UserCreate, request: Request):
    client = _client(request)
    user_id = uuid.uuid4().hex
    await client.put(_key(user_id), body.model_dump())
    _, meta, bins = await client.get(_key(user_id))
    return UserResponse(user_id=user_id, generation=meta.gen, **bins)

@router.get("/{user_id}", response_model=UserResponse)
async def get_user(user_id: str, request: Request):
    client = _client(request)
    try:
        _, meta, bins = await client.get(_key(user_id))
    except RecordNotFound:
        raise HTTPException(status_code=404, detail="User not found")
    return UserResponse(user_id=user_id, generation=meta.gen, **bins)

@router.delete("/{user_id}", status_code=204)
async def delete_user(user_id: str, request: Request):
    client = _client(request)
    try:
        await client.remove(_key(user_id))
    except RecordNotFound:
        raise HTTPException(status_code=404, detail="User not found")
```

#### Full Example

The [`examples/sample-fastapi/`](https://github.com/KimSoungRyoul/aerospike-py/tree/main/examples/sample-fastapi) directory contains a complete application with 11 routers, Pydantic models, Docker Compose setup, and tests.

```bash
cd examples/sample-fastapi
docker compose up -d
pip install -r requirements.txt
uvicorn app.main:app --reload
# Visit http://localhost:8000/docs
```


---

### Logging

> Rust-to-Python logging bridge for observing Aerospike client internals.

Built-in **Rust-to-Python logging bridge** that forwards all internal Rust logs to Python's `logging` module. Initialized automatically on import.

#### Quick Start

```python
import logging
import aerospike_py

logging.basicConfig(level=logging.DEBUG)

client = aerospike_py.client({"hosts": [("127.0.0.1", 3000)]}).connect()
# DEBUG:aerospike_core::cluster: Connecting to seed 127.0.0.1:3000
```

#### Log Level Control

```python
aerospike_py.set_log_level(aerospike_py.LOG_LEVEL_DEBUG)
```

| Constant | Value | Python Level |
|---|---|---|
| `LOG_LEVEL_OFF` | -1 | (disabled) |
| `LOG_LEVEL_ERROR` | 0 | ERROR (40) |
| `LOG_LEVEL_WARN` | 1 | WARNING (30) |
| `LOG_LEVEL_INFO` | 2 | INFO (20) |
| `LOG_LEVEL_DEBUG` | 3 | DEBUG (10) |
| `LOG_LEVEL_TRACE` | 4 | TRACE (5) |

#### Logger Names

| Logger | Description |
|---|---|
| `aerospike_core::cluster` | Cluster discovery, node management |
| `aerospike_core::batch` | Batch operation execution |
| `aerospike_core::command` | Individual command execution |
| `aerospike_py` | Python-side client wrapper |

```python
# Fine-grained control
logging.getLogger("aerospike_core::cluster").setLevel(logging.DEBUG)
logging.getLogger("aerospike_core::batch").setLevel(logging.WARNING)
```

#### JSON Logging

```python
import logging, json

class JSONFormatter(logging.Formatter):
    def format(self, record):
        return json.dumps({
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
        })

handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger = logging.getLogger("aerospike_core")
logger.addHandler(handler)
logger.setLevel(logging.DEBUG)
```

#### Framework Integration

##### FastAPI

```python
import logging
from contextlib import asynccontextmanager
import aerospike_py
from fastapi import FastAPI

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(name)s %(levelname)s %(message)s")

@asynccontextmanager
async def lifespan(app: FastAPI):
    aerospike_py.set_log_level(aerospike_py.LOG_LEVEL_INFO)
    client = aerospike_py.AsyncClient({"hosts": [("127.0.0.1", 3000)]})
    await client.connect()
    app.state.aerospike = client
    yield
    await client.close()

app = FastAPI(lifespan=lifespan)
```

##### Django

```python
# settings.py
LOGGING = {
    "version": 1,
    "handlers": {"console": {"class": "logging.StreamHandler"}},
    "loggers": {
        "aerospike_core": {"handlers": ["console"], "level": "INFO"},
        "aerospike_py": {"handlers": ["console"], "level": "INFO"},
    },
}
```

#### File Logging

```python
import logging

handler = logging.FileHandler("aerospike.log")
handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(name)s %(message)s"))

for name in ["aerospike_core", "aerospike_py"]:
    logger = logging.getLogger(name)
    logger.addHandler(handler)
    logger.setLevel(logging.DEBUG)
```

#### Disabling

```python
aerospike_py.set_log_level(aerospike_py.LOG_LEVEL_OFF)
```


---

### Prometheus Metrics

> Prometheus metrics for monitoring Aerospike operations.

aerospike-py collects operation-level metrics in Rust and exposes them in **Prometheus text format**. Metric names follow [OpenTelemetry DB Client Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/database/).

#### Quick Start

```python
import aerospike_py

# Get metrics as string
text: str = aerospike_py.get_metrics()

# Or start a built-in HTTP server
aerospike_py.start_metrics_server(port=9464)
# Prometheus scrapes http://localhost:9464/metrics

# Stop when done
aerospike_py.stop_metrics_server()
```

#### `db_client_operation_duration_seconds`

A **histogram** tracking the duration of every data operation.

**Labels:**

| Label | Examples |
|---|---|
| `db_system_name` | `aerospike` |
| `db_namespace` | `test`, `production` |
| `db_collection_name` | `users`, `sessions` |
| `db_operation_name` | `get`, `put`, `delete`, `query` |
| `error_type` | `""` (success), `Timeout`, `KeyNotFoundError` |

**Buckets:** `0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0` seconds

**Instrumented operations:** `put`, `get`, `select`, `exists`, `remove`, `touch`, `append`, `prepend`, `increment`, `operate`, `batch_read`, `batch_operate`, `batch_remove`, `query`

> **Tip:**
> `exists()` treats `KeyNotFoundError` as success since "not found" is a normal outcome.
#### Framework Integration

##### FastAPI

```python
from fastapi import FastAPI, Response
from prometheus_client import generate_latest, REGISTRY
import aerospike_py

@app.get("/metrics")
def metrics():
    python_metrics = generate_latest(REGISTRY).decode("utf-8")
    aerospike_metrics = aerospike_py.get_metrics()
    return Response(
        python_metrics + "\n" + aerospike_metrics,
        media_type="text/plain; version=0.0.4",
    )
```

##### Django

```python
# myproject/apps.py
from django.apps import AppConfig
import aerospike_py

class MyAppConfig(AppConfig):
    name = "myapp"

    def ready(self):
        aerospike_py.start_metrics_server(port=9464)
```

#### Prometheus Config

```yaml
scrape_configs:
  - job_name: "aerospike-py"
    scrape_interval: 15s
    static_configs:
      - targets: ["localhost:9464"]
```

#### PromQL Examples

```promql
# Average latency (5m)
rate(db_client_operation_duration_seconds_sum[5m])
/ rate(db_client_operation_duration_seconds_count[5m])

# P99 latency
histogram_quantile(0.99, rate(db_client_operation_duration_seconds_bucket[5m]))

# Error rate by type
sum by (error_type) (rate(db_client_operation_duration_seconds_count{error_type!=""}[5m]))

# Ops/sec by namespace
sum by (db_namespace, db_operation_name) (rate(db_client_operation_duration_seconds_count[1m]))
```

#### Grafana Dashboard

| Panel | PromQL | Type |
|---|---|---|
| Ops/sec | `sum(rate(..._count[1m])) by (db_operation_name)` | Time series |
| P50/P95/P99 | `histogram_quantile(0.5\|0.95\|0.99, rate(..._bucket[5m]))` | Time series |
| Error Rate | `sum(rate(..._count{error_type!=""}[1m])) by (error_type)` | Time series |
| By Namespace | `sum(rate(..._count[1m])) by (db_namespace)` | Pie chart |

#### Performance

| Scenario | Overhead |
|---|---|
| Per-operation recording | ~30-80 ns (atomic increment) |
| Relative to network round-trip | 0.001-0.01% |
| `get_metrics()` encoding | ~50-200 us |

Metrics collection is always enabled with negligible overhead.


---

### Distributed Tracing

> OpenTelemetry distributed tracing for Aerospike operations.

Built-in **OpenTelemetry tracing** for every data operation. Spans follow [Database Client Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/database/) and export via **OTLP gRPC**.

#### Quick Start

```bash
pip install aerospike-py            # tracing built-in
pip install aerospike-py[otel]      # + context propagation from Python spans
```

```python
import aerospike_py

# 1. Initialize
aerospike_py.init_tracing()

# 2. Use client -- all operations are traced automatically
client = aerospike_py.client({"hosts": [("127.0.0.1", 3000)]}).connect()
client.put(("test", "users", "user1"), {"name": "Alice"})
client.get(("test", "users", "user1"))
client.close()

# 3. Flush pending spans before exit
aerospike_py.shutdown_tracing()
```

#### API

| Function | Description |
|---|---|
| `init_tracing()` | Initialize OTLP tracer. Reads `OTEL_*` env vars. |
| `shutdown_tracing()` | Flush and shut down. Call before process exit. |

Both are thread-safe and idempotent.

#### Environment Variables

| Variable | Default | Description |
|---|---|---|
| `OTEL_EXPORTER_OTLP_ENDPOINT` | `http://localhost:4317` | OTLP gRPC endpoint |
| `OTEL_SERVICE_NAME` | `aerospike-py` | Service name |
| `OTEL_SDK_DISABLED` | `false` | Disable tracing entirely |
| `OTEL_TRACES_EXPORTER` | `otlp` | Set to `none` to disable export |

#### Span Attributes

| Attribute | Example |
|---|---|
| `db.system.name` | `aerospike` |
| `db.namespace` | `test` |
| `db.collection.name` | `users` |
| `db.operation.name` | `PUT`, `GET`, `REMOVE` |

**Span name:** `{OPERATION} {namespace}.{set}` (e.g., `PUT test.users`)

**On error:** `error.type`, `db.response.status_code`, `otel.status_code=ERROR`

**Instrumented:** `put`, `get`, `select`, `exists`, `remove`, `touch`, `append`, `prepend`, `increment`, `operate`, `batch_read`, `batch_operate`, `batch_remove`, `query`

#### Context Propagation

With `aerospike-py[otel]` installed, W3C TraceContext is automatically propagated from Python active spans to Rust spans:

| Setup | Behavior |
|---|---|
| `aerospike-py[otel]` + active span | Python span becomes parent |
| `aerospike-py[otel]` + no active span | Root span created |
| `aerospike-py` (base) | Root span (no propagation) |

#### Framework Integration

##### FastAPI

```python
from contextlib import asynccontextmanager
import aerospike_py
from fastapi import FastAPI

@asynccontextmanager
async def lifespan(app: FastAPI):
    aerospike_py.init_tracing()
    client = aerospike_py.AsyncClient({"hosts": [("127.0.0.1", 3000)]})
    await client.connect()
    app.state.aerospike = client
    yield
    await client.close()
    aerospike_py.shutdown_tracing()

app = FastAPI(lifespan=lifespan)
```

For end-to-end HTTP-to-Aerospike traces:

```bash
pip install opentelemetry-instrumentation-fastapi
```

```python
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
FastAPIInstrumentor.instrument_app(app)
```

##### Django

```python
# apps.py
from django.apps import AppConfig
import aerospike_py

class MyAppConfig(AppConfig):
    name = "myapp"
    def ready(self):
        aerospike_py.init_tracing()

# settings.py
import atexit, aerospike_py
atexit.register(aerospike_py.shutdown_tracing)
```

#### Jaeger Setup

```bash
docker run -d --name jaeger \
  -p 4317:4317 -p 16686:16686 \
  jaegertracing/all-in-one:latest

export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
export OTEL_SERVICE_NAME=my-aerospike-app
```

Visit `http://localhost:16686` to view traces.

#### Disabling Tracing

```bash
export OTEL_SDK_DISABLED=true          # disable entirely
export OTEL_TRACES_EXPORTER=none       # spans created but not exported
```

#### Graceful Degradation

| Scenario | Behavior |
|---|---|
| OTLP endpoint unreachable | Warning log, tracing disabled |
| `init_tracing()` not called | No-op spans |
| `opentelemetry-api` not installed | Root spans (no propagation) |
| `shutdown_tracing()` not called | Some pending spans may be lost |

#### Performance

| Scenario | Overhead |
|---|---|
| Span creation | ~1-5 us |
| Context propagation | ~10-50 us |
| vs network round-trip | < 1% |
| `OTEL_SDK_DISABLED=true` | ~30-80 ns (metrics only) |


---

## Performance

### Performance Overview

#### Architecture

> **Diagram:** Flowchart showing data flow between Python Application, PyO3 Extension, Tokio Runtime (GIL-free), OS Kernel.

```mermaid
flowchart TD
  subgraph APP["Python Application"]
    Client["Client (sync)"]
    AsyncClient["AsyncClient (async)"]
  end

  subgraph FFI["PyO3 Extension"]
    Sync["py.detach + block_on"]
    Async["future_into_py"]
  end

  subgraph RT["Tokio Runtime (GIL-free)"]
    Core["aerospike-core + Pool"]
    NP["NumPy zero-copy"]
  end

  subgraph OS["OS Kernel"]
    TCP["TCP Socket"]
  end

  DB[("Aerospike Cluster")]

  Client --> Sync
  AsyncClient --> Async
  Sync --> Core
  Async --> Core
  Core -.-> NP
  Core --> TCP --> DB

  style APP fill:#3776ab,color:#fff
  style FFI fill:#dea584,color:#000
  style RT fill:#ce422b,color:#fff
  style OS fill:#555,color:#fff
  style DB fill:#00bfa5,color:#fff
```

##### Key Design

- **GIL release**: Both clients release GIL before Rust I/O. Sync uses `py.detach()` + `RUNTIME.block_on()`, async uses `future_into_py()`
- **Single Tokio runtime**: Global multi-threaded runtime shared across all clients. Worker threads = CPU cores
- **Zero-copy NumPy**: `batch_read(..., _dtype=dtype)` writes directly into numpy buffer via raw pointers -- no intermediate Python objects
- **Connection pooling**: Managed by `aerospike-core` with configurable `max_conns_per_node` and `idle_timeout`

#### Benchmark Methodology

1. **Warmup phase** -- results excluded
2. **Multiple rounds** -- median of medians
3. **Pre-seeded data** for reads
4. **GC disabled** during measurement
5. **Isolated key prefixes** per client

#### Comparison

| Client | Runtime |
|--------|---------|
| aerospike-py (sync) | Rust + Python |
| aerospike-py (async) | Rust + Python |
| official aerospike | C + Python |

#### Running Benchmarks

```bash
make run-benchmark                # Console output
make run-benchmark-report         # MD report + SVG charts
```

```bash
# Custom parameters
make run-benchmark BENCH_COUNT=10000 BENCH_ROUNDS=30 BENCH_CONCURRENCY=100
```

| Parameter | Default | Description |
|-----------|---------|-------------|
| `BENCH_COUNT` | 5,000 | Operations per round |
| `BENCH_ROUNDS` | 20 | Rounds per operation |
| `BENCH_CONCURRENCY` | 50 | Async concurrency |
| `BENCH_BATCH_GROUPS` | 10 | Batch read groups |

#### Results

- [Benchmark Results](/docs/performance/benchmark-results)
- [NumPy Batch Benchmark](/docs/performance/numpy-benchmark-results)


---
